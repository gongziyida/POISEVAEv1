{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import gibbs_sampler_poise\n",
    "# import kl_divergence_calculator\n",
    "from numpy import prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def _latent_dims_type_setter(lds):\n",
    "    ret, ret_flatten = [], []\n",
    "    for ld in lds:\n",
    "        if hasattr(ld, '__iter__'): # Iterable\n",
    "            ld_tuple = tuple([i for i in ld])\n",
    "            if not all(map(lambda i: isinstance(i, int), ld_tuple)):\n",
    "                raise ValueError('`latent_dim` must be either iterable of ints or int.')\n",
    "            ret.append(ld_tuple)\n",
    "            ret_flatten.append(int(prod(ld_tuple)))\n",
    "        elif isinstance(ld, int):\n",
    "            ret.append((ld, ))\n",
    "            ret_flatten.append(ld)\n",
    "        else:\n",
    "            raise ValueError('`latent_dim` must be either iterable of ints or int.')\n",
    "    return ret, ret_flatten\n",
    "\n",
    "\n",
    "class POISEVAE(nn.Module):\n",
    "    __version__ = 1.0\n",
    "    \n",
    "    def __init__(self, encoders, decoders, batch_size, latent_dims=None, use_mse_loss=True,\n",
    "                 device=_device):\n",
    "        \"\"\"\n",
    "        encoders: list of nn.Module\n",
    "            Each encoder must have an attribute `latent_dim` specifying the dimension of the\n",
    "            latent space to which it encodes. An alternative way to avoid adding this attribute\n",
    "            is to specify the `latent_dims` parameter (see below). \n",
    "            Note that each `latent_dim` must be unsqueezed, e.g. (10, ) is not the same as (10, 1).\n",
    "            \n",
    "        decoders: list of nn.Module\n",
    "            The number and indices of decoders must match those of encoders.\n",
    "            \n",
    "        batch_size: int\n",
    "        \n",
    "        latent_dims: iterable, optional; default None\n",
    "            The dimensions of the latent spaces to which the encoders encode. The indices of the \n",
    "            entries must match those of encoders. An alternative way to specify the dimensions is\n",
    "            to add the attribute `latent_dim` to each encoder (see above).\n",
    "            Note that each entry must be unsqueezed, e.g. (10, ) is not the same as (10, 1).\n",
    "        \n",
    "        use_mse_loss: boolean, optional; default True\n",
    "            To use MSE loss or not; if not, BCE loss will be used.\n",
    "        \n",
    "        device: torch.device, optional\n",
    "        \"\"\"\n",
    "        super(POISEVAE,self).__init__()\n",
    "\n",
    "        if len(encoders) != len(decoders):\n",
    "            raise ValueError('The number of encoders must match that of decoders.')\n",
    "        \n",
    "        if len(encoders) > 2:\n",
    "            raise NotImplementedError('> 3 latent spaces not yet supported.')\n",
    "        \n",
    "        # Type check\n",
    "        if not all(map(lambda x: isinstance(x, nn.Module), (*encoders, *decoders))):\n",
    "            raise TypeError('`encoders` and `decoders` must be lists of `nn.Module` class.')\n",
    "\n",
    "        # Get the latent dimensions\n",
    "        if latent_dims is not None:\n",
    "            if not hasattr(latent_dims, '__iter__'): # Iterable\n",
    "                raise TypeError('`latent_dims` must be iterable.')\n",
    "            self.latent_dims = latent_dims\n",
    "        else:\n",
    "            self.latent_dims = tuple(map(lambda l: l.latent_dim, encoders))\n",
    "        self.latent_dims, self.latent_dims_flatten = _latent_dims_type_setter(self.latent_dims)\n",
    "\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.decoders = nn.ModuleList(decoders)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.gibbs = gibbs_sampler(self.latent_dims_flatten, batch_size)\n",
    "        self.kl_div = kl_divergence(self.latent_dims_flatten, batch_size)\n",
    "\n",
    "        self.register_parameter(name='g11', \n",
    "                                param=nn.Parameter(torch.randn(*self.latent_dims_flatten, \n",
    "                                                               device=self.device)))\n",
    "        self.register_parameter(name='g22', \n",
    "                                param=nn.Parameter(torch.randn(*self.latent_dims_flatten, \n",
    "                                                               device=self.device)))\n",
    "        self.flag_initialize = 1\n",
    "\n",
    "    def _decoder_helper(self):\n",
    "        \"\"\"\n",
    "        Reshape samples drawn from each latent space, and decode with considering the loss function\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        for decoder, z, ld in zip(self.decoders, self.z_gibbs_posteriors, self.latent_dims):\n",
    "            z = z.view(self.batch_size, *ld) # Match the shape to the output\n",
    "            x_ = decoder(z)\n",
    "            if not self.use_mse_loss: # BCE instead\n",
    "                x_ = torch.sigmoid(x_)\n",
    "            ret.append(x_)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, var = [], []\n",
    "        for i, xi in enumerate(x):\n",
    "            _mu, _log_var = self.encoders[i].forward(xi)\n",
    "            mu.append(_mu.view(self.batch_size, -1))\n",
    "            var.append(-torch.exp(_log_var.view(self.batch_size, -1)))\n",
    "\n",
    "        g22 = -torch.exp(self.g22)\n",
    "\n",
    "        # Initializing gibbs sample\n",
    "        if self.flag_initialize == 1:\n",
    "            z_priors = self.gibbs.sample(self.g11, g22, n_iterations=5000)\n",
    "            z_posteriors = self.gibbs.sample(self.g11, g22, lambda1s=mu, lambda2s=var,\n",
    "                                             n_iterations=5000)\n",
    "\n",
    "            self.z_priors = z_priors\n",
    "            self.z_posteriors = z_posteriors\n",
    "            self.flag_initialize = 0\n",
    "\n",
    "        z_priors = list(map(lambda z: z.detach(), self.z_priors))\n",
    "        z_posteriors = list(map(lambda z: z.detach(), self.z_posteriors))\n",
    "\n",
    "        # If lambda not provided, treat as zeros to save memory and computation\n",
    "        self.z_gibbs_priors = self.gibbs.sample(self.g11, g22, z=z_priors, n_iterations=5)\n",
    "        self.z_gibbs_posteriors = self.gibbs.sample(self.g11, g22, lambda1s=mu, lambda2s=var,\n",
    "                                                    z=z_posteriors, n_iterations=5)\n",
    "\n",
    "        self.z_priors = list(map(lambda z: z.detach(), self.z_gibbs_priors))\n",
    "        self.z_posteriors = list(map(lambda z: z.detach(), self.z_gibbs_posteriors))\n",
    "\n",
    "        G = torch.block_diag(self.g11, self.g22)\n",
    "\n",
    "        x_ = self._decoder_helper() # Decoding\n",
    "\n",
    "        # self.z2_gibbs_posterior = self.z2_gibbs_posterior.squeeze()\n",
    "        for i in range(len(self.z_gibbs_posteriors)):\n",
    "            self.z_gibbs_posteriors[i] = self.z_gibbs_posteriors[i].squeeze()\n",
    "\n",
    "        # KL loss\n",
    "        kls = self.kl_div.calc(G, self.z_gibbs_posteriors, self.z_gibbs_priors, mu,var)\n",
    "        KL_loss  = sum(kls)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        loss_func = nn.MSELoss(reduction='sum') if self.use_mse_loss else nn.BCELoss(reduction='sum')\n",
    "        recs = list(map(lambda x: loss_func(x[0], x[1]), zip(x_, x)))\n",
    "        rec_loss = sum(recs)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = KL_loss + rec_loss\n",
    "\n",
    "        return self.z_posteriors, x_, mu, var, loss, recs, KL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kl_divergence():\n",
    "    __version__ = 1.0\n",
    "    \n",
    "    def __init__(self, latent_dims, batch_size, device=_device):\n",
    "        self.latent_dims = latent_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def calc(self, G, z, z_priors, mu, var):\n",
    "        ## Creating Sufficient statistics\n",
    "        T_priors, T_posts, lambdas = [], [], []\n",
    "        for z_i, z_prior_i, mu_i, var_i in zip(z, z_priors, mu, var):\n",
    "            T_priors.append(torch.cat((z_prior_i, torch.square(z_prior_i)), 1))\n",
    "            T_posts.append(torch.cat((z_i, torch.square(z_i)), 1))\n",
    "            lambdas.append(torch.cat((mu_i,var_i),1))\n",
    "            \n",
    "        # TODO: make it generic for > 2 latent spaces\n",
    "        T_prior_sqrd = torch.sum(torch.square(z_priors[0]), 1) + \\\n",
    "                       torch.sum(torch.square(z_priors[1]), 1) #stores z^2+z'^2\n",
    "        T_post_sqrd  = torch.sum(torch.square(z[0]), 1) + \\\n",
    "                       torch.sum(torch.square(z[1]), 1)\n",
    "        T1_prior_unsq = T_priors[0].unsqueeze(2)       \n",
    "        T2_prior_unsq = T_priors[1].unsqueeze(1)       \n",
    "        T1_post_unsq  = T_posts[0].unsqueeze(2)        \n",
    "        T2_post_unsq  = T_posts[1].unsqueeze(1)        \n",
    "        T_prior_kron = torch.zeros(self.batch_size, 2 * self.latent_dims[0], \n",
    "                                   2 * self.latent_dims[1]).to(self.device)\n",
    "        T_post_kron = torch.zeros(T_prior_kron.shape).to(self.device)\n",
    "       \n",
    "        for i in range(self.batch_size):\n",
    "            T_prior_kron[i,:] = torch.kron(T1_prior_unsq[i,:], T2_prior_unsq[i,:])\n",
    "            T_post_kron[i,:] = torch.kron(T1_post_unsq[i,:], T2_post_unsq[i,:])    \n",
    "            \n",
    "        part_fun0 = self.dot_product(lambdas[0], T_posts[0]) + \\\n",
    "                    self.dot_product(lambdas[1], T_posts[1])\n",
    "        part_fun1 = -self.dot_product(lambdas[0], T_posts[0].detach()) - \\\n",
    "                     self.dot_product(lambdas[1], T_posts[1].detach()) #-lambda*Tq-lambda'Tq'    \n",
    "        part_fun2 = self.dot_product(T_prior_kron.detach(), G) - \\\n",
    "                    self.dot_product(T_post_kron.detach(), G)\n",
    "\n",
    "        return part_fun0, part_fun1, part_fun2\n",
    "    \n",
    "    def dot_product(self, tensor_1, tensor_2):\n",
    "        out = torch.sum(torch.mul(tensor_1, tensor_2))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gibbs_sampler():\n",
    "    __version__ = 1.0\n",
    "    \n",
    "    def __init__(self, latent_dims, batch_size, device=_device):\n",
    "        self.latent_dims = latent_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def var_calc(self,z, g22, lambda_2):\n",
    "        val = 1 - torch.matmul(torch.square(z), g22)\n",
    "        if lambda_2 is not None:\n",
    "            val -= lambda_2\n",
    "        return torch.reciprocal(2 * val)\n",
    "\n",
    "    def mean_calc(self, z, var, g11, lambda_1):\n",
    "        beta = torch.matmul(z, g11)\n",
    "        if lambda_1 is not None:\n",
    "            beta += lambda_1\n",
    "        return var * beta\n",
    "\n",
    "    def value_calc(self,z, g11, g22, lambda_1, lambda_2):\n",
    "        var1 = self.var_calc(z, g22, lambda_2)\n",
    "        mean1 = self.mean_calc(z, var1, g11, lambda_1)\n",
    "        out = mean1 + torch.sqrt(var1.float()) * torch.randn_like(var1)\n",
    "        return out\n",
    "\n",
    "    def sample(self, g11, g22, z=None, lambda1s=None, lambda2s=None, n_iterations=1):\n",
    "        \"\"\"\n",
    "        g11, g22: \n",
    "            Diagonal blocks of the metric tensor\n",
    "        z: \n",
    "            If not provided, randomly initialize\n",
    "        lambda1s: optional\n",
    "            Natural parameter 1 of the latent distributions\n",
    "            If not provided, treat as zeros\n",
    "        lambda1s: optional\n",
    "            Natural parameter 2 of the latent distributions\n",
    "            If not provided, treat as zeros\n",
    "        n_iterations: int, optional; default 1\n",
    "        \"\"\"\n",
    "            # TODO: function signature of gibbs_sample: optional parameters\n",
    "            # flag_init. not necessary; if z not provided, init. z rand.ly\n",
    "            # Not really an optimization but make the code clear\n",
    "            # in case people want to look carefully in the future\n",
    "            # I made an attempt in the local file `gibbs_sampler_poise.py`; debugging needed\n",
    "        if z is None:\n",
    "            z = [torch.randn(self.batch_size, ld).squeeze().to(self.device) \n",
    "                 for ld in self.latent_dims]\n",
    "        if lambda1s is None:\n",
    "            lambda1s = [None for _ in range(len(self.latent_dims))]\n",
    "        if lambda2s is None:\n",
    "            lambda2s = [None for _ in range(len(self.latent_dims))]\n",
    "\n",
    "        # TODO: make it generic for > 2 latent spaces \n",
    "        for i in range(n_iterations):\n",
    "            z[0] = self.value_calc(z[1], torch.transpose(g11,0,1), torch.transpose(g22,0,1),\n",
    "                                   lambda1s[0], lambda2s[0]) \n",
    "            z[1] = self.value_calc(z[0], g11, g22, lambda1s[1], lambda2s[1])\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class Encoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.l1 = nn.Linear(100, 50).to(_device)\n",
    "        self.l2mu = nn.Linear(50, 10).to(_device)\n",
    "        self.l2var = nn.Linear(50, 10).to(_device)\n",
    "        self.latent_dim = 10\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        mu = self.l2mu(x)\n",
    "        log_var = self.l2var(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "class Encoder2(nn.Module):\n",
    "    # 64*64 -> 40*40 -> 16*16 -> 4*4\n",
    "    def __init__(self):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.l1 = nn.Conv2d(3, 2, (25, 25)).to(_device)\n",
    "        self.l2 = nn.Conv2d(2, 1, (25, 25)).to(_device)\n",
    "        self.l2mu = nn.Conv2d(1, 1, (13, 13)).to(_device)\n",
    "        self.l2var = nn.Conv2d(1, 1, (13, 13)).to(_device)\n",
    "        self.latent_dim = (1, 4, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        mu = self.l2mu(x)\n",
    "        log_var = self.l2var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "enc1 = Encoder1()\n",
    "dec1 = nn.Sequential(nn.Linear(10, 50), nn.Linear(50, 100)).to(_device)\n",
    "\n",
    "\n",
    "enc2 = Encoder2()\n",
    "dec2 = nn.Sequential(nn.ConvTranspose2d(1, 1, (13, 13)), \n",
    "                     nn.ConvTranspose2d(1, 2, (25, 25)), \n",
    "                     nn.ConvTranspose2d(2, 3, (25, 25))).to(_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = POISEVAE([enc1, enc2], [dec1, dec2], batch_size=10, use_mse_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g11\n",
      "Parameter containing:\n",
      "tensor([[ 0.8727,  0.3870, -1.1587,  0.7343, -0.8992,  2.4151, -1.2597, -0.4085,\n",
      "          1.6263, -0.4045, -0.2168, -1.6195,  0.4576, -0.5562, -0.5924,  0.5054],\n",
      "        [-0.5479,  0.5559, -0.5882, -0.4353,  0.0805,  0.8349, -1.1391, -0.6640,\n",
      "         -0.3031, -0.2652,  0.2191, -0.1638, -0.9558, -1.1803, -1.2815,  1.0159],\n",
      "        [-0.0364,  0.6966,  2.1464, -0.4292,  0.9246,  0.2827,  0.5120, -1.8514,\n",
      "          0.6269, -0.1945,  0.7781, -0.1943,  0.6113, -0.7763,  0.2902, -0.3342],\n",
      "        [ 0.3187, -0.3913, -0.7303, -0.5035, -0.2646, -1.5611,  0.2867, -1.2421,\n",
      "         -0.1090,  0.7692,  0.4332,  1.7634, -0.4913, -1.4558, -1.0980,  0.7011],\n",
      "        [ 0.2584,  0.2169,  0.6178,  0.0270, -0.7365, -0.0467,  0.7720, -0.7526,\n",
      "          0.7086, -0.2269,  0.0050,  1.5981, -1.2214, -0.7192, -0.5574, -0.4297],\n",
      "        [-0.1948,  0.9114,  0.2141,  1.0081, -1.7902, -1.5355,  1.7394,  1.6129,\n",
      "         -4.4753, -0.5901, -0.4308,  0.6687,  0.6133, -0.8787, -0.2540, -0.0594],\n",
      "        [ 0.2757,  0.5819, -3.0778,  0.3913,  2.1481,  0.9958,  0.2380,  0.2909,\n",
      "          1.5863, -0.7433, -0.8658, -0.5215, -0.7907,  0.2023,  1.0696, -1.3611],\n",
      "        [-1.0474,  1.3242, -0.1196,  0.5311,  1.0490,  1.2651, -1.4441, -1.1149,\n",
      "          1.9134, -1.7275,  0.0943,  1.8870,  0.8475, -1.7654,  0.4167,  0.2983],\n",
      "        [-0.1572, -0.2340, -1.2604, -0.9807, -0.2894,  0.4061, -0.1870, -0.8442,\n",
      "          0.0276,  0.9910,  1.2685,  0.7806,  1.0282, -0.7119,  1.0779, -0.7278],\n",
      "        [-0.1915,  0.3117, -0.6924,  0.2464,  0.0974, -0.5958,  0.3553, -0.2061,\n",
      "         -1.6118, -0.4509, -0.6589, -1.6955, -0.0301, -0.2650,  0.5441,  0.9585]],\n",
      "       requires_grad=True)\n",
      "g22\n",
      "Parameter containing:\n",
      "tensor([[-2.2029,  0.0936,  0.2193, -0.9615, -1.5916, -2.3308,  1.0387, -0.0915,\n",
      "          1.3997, -0.7432,  0.9895,  0.3871, -1.1425, -2.3793, -1.6533,  0.7637],\n",
      "        [ 0.8435, -1.7563, -0.9537, -1.1641,  1.2225,  0.0116, -1.7368, -0.5897,\n",
      "         -0.4634, -1.1108,  0.6420,  1.2342,  1.2543,  0.9341, -1.0627, -0.7161],\n",
      "        [ 0.0594, -0.1347, -0.7614,  0.0507,  1.8449,  0.8150,  0.3346, -1.1580,\n",
      "         -1.0576,  0.5962,  1.4099,  2.5583, -0.1117,  1.3956,  0.4345, -0.5169],\n",
      "        [-1.2162,  1.2308, -0.8387, -0.1505,  0.2889,  0.8243,  0.3237,  0.6588,\n",
      "         -0.6419, -1.0639, -0.5949, -1.6997,  1.5295, -0.3544, -0.9971, -0.2015],\n",
      "        [ 0.2599, -0.6271, -1.3562, -0.3270,  0.6314, -0.2745,  1.0379,  0.2131,\n",
      "         -0.3100, -1.2535, -2.2802,  0.8117,  0.1081,  2.7173,  0.2894, -1.6241],\n",
      "        [ 0.5154,  0.0504,  1.9235, -0.0105, -0.8199,  0.4817,  2.0997,  0.0146,\n",
      "         -1.7167,  0.4693,  0.3395, -0.7411,  0.4430, -0.3495, -0.7026, -1.4680],\n",
      "        [ 0.5517,  0.1586,  1.5034,  0.2704, -1.1033,  0.1747,  0.6146, -0.0462,\n",
      "         -0.1993, -0.6542,  2.3024,  0.1530, -0.7934, -0.8294,  0.1786, -1.2484],\n",
      "        [ 0.8333, -0.8593,  0.0199, -0.6990, -0.0731,  0.3630,  0.2267,  1.2913,\n",
      "          1.2034,  0.4595,  0.4632,  0.9092,  0.7079, -1.1872, -0.9414,  1.0043],\n",
      "        [-0.3666,  3.2564, -0.6010,  0.8201,  0.6373, -0.4895,  2.4594, -0.3351,\n",
      "         -1.2286,  0.2253, -1.8502,  0.8018,  0.5488, -0.7901, -0.2243,  1.1735],\n",
      "        [-2.1838,  0.7051, -1.2611,  1.4157, -0.4028, -0.6381,  0.2674,  0.8520,\n",
      "          1.3717, -0.2209, -0.1528,  0.5576,  1.1948, -1.1755, -0.6652, -0.7918]],\n",
      "       requires_grad=True)\n",
      "encoders.0.l1.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0733, -0.0149, -0.0334,  ..., -0.0653, -0.0979, -0.0162],\n",
      "        [ 0.0525, -0.0735,  0.0581,  ...,  0.0195,  0.0301,  0.0040],\n",
      "        [-0.0552,  0.0814, -0.0927,  ..., -0.0097,  0.0043,  0.0772],\n",
      "        ...,\n",
      "        [-0.0492,  0.0757, -0.0193,  ..., -0.0706, -0.0661, -0.0427],\n",
      "        [ 0.0703,  0.0477,  0.0499,  ..., -0.0421, -0.0396,  0.0103],\n",
      "        [ 0.0189, -0.0888,  0.0393,  ...,  0.0200, -0.0204,  0.0105]],\n",
      "       requires_grad=True)\n",
      "encoders.0.l1.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0122,  0.0486,  0.0375, -0.0555, -0.0385, -0.0338,  0.0319, -0.0240,\n",
      "        -0.0217, -0.0793,  0.0602,  0.0204, -0.0489, -0.0739, -0.0419, -0.0748,\n",
      "        -0.0306, -0.0187, -0.0646,  0.0869, -0.0289, -0.0967, -0.0101, -0.0692,\n",
      "         0.0854, -0.0639, -0.0201, -0.0615,  0.0022, -0.0200,  0.0382, -0.0869,\n",
      "         0.0045,  0.0735, -0.0029,  0.0341, -0.0637,  0.0166,  0.0889,  0.0763,\n",
      "        -0.0153, -0.0834,  0.0874,  0.0042, -0.0030, -0.0243,  0.0292, -0.0214,\n",
      "        -0.0962,  0.0771], requires_grad=True)\n",
      "encoders.0.l2mu.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1386,  0.1270, -0.0129,  0.1349, -0.1173,  0.0118, -0.0619, -0.1097,\n",
      "         -0.0105,  0.0667,  0.0184, -0.0513, -0.0850,  0.0908,  0.1301,  0.0154,\n",
      "         -0.0738, -0.0715,  0.1304, -0.0462,  0.0929, -0.0595, -0.1405,  0.0917,\n",
      "         -0.0313, -0.1011, -0.0756, -0.0960,  0.1003,  0.0728,  0.1168,  0.0309,\n",
      "          0.0025, -0.1214, -0.1076,  0.0121,  0.0930, -0.0100,  0.0705, -0.0764,\n",
      "         -0.0877,  0.1246, -0.0163, -0.0049, -0.0311,  0.1187, -0.0433,  0.0568,\n",
      "          0.0106, -0.1357],\n",
      "        [-0.1293,  0.0257, -0.0044, -0.1007,  0.0790,  0.0323,  0.0182,  0.0833,\n",
      "          0.1414,  0.1160, -0.0116, -0.1089, -0.0622,  0.1240, -0.1159,  0.1094,\n",
      "         -0.0124, -0.0959, -0.0657, -0.1301, -0.1307,  0.1219, -0.0213,  0.1219,\n",
      "          0.0110, -0.1153, -0.1145, -0.0365, -0.1036, -0.1154,  0.1002,  0.0369,\n",
      "         -0.1229,  0.0244, -0.0488,  0.0661,  0.0800, -0.0584, -0.1044,  0.0810,\n",
      "         -0.0619,  0.0563,  0.0680,  0.1133, -0.1358,  0.1113,  0.0288,  0.1072,\n",
      "          0.0421, -0.0141],\n",
      "        [ 0.0568, -0.0639,  0.0128,  0.0776,  0.0428, -0.0342, -0.0774,  0.0305,\n",
      "         -0.1124, -0.0008,  0.0871, -0.1090,  0.0741,  0.0192, -0.0557, -0.0338,\n",
      "          0.1116,  0.0777, -0.0098,  0.0612,  0.0153,  0.0932,  0.0077, -0.1054,\n",
      "          0.0120, -0.1345,  0.1082,  0.0559,  0.0523, -0.1011, -0.0733, -0.0319,\n",
      "          0.0931, -0.0593, -0.0950,  0.0824, -0.0504,  0.0984, -0.0985,  0.1078,\n",
      "         -0.0516,  0.0603,  0.0669,  0.0129, -0.0991,  0.0376,  0.0235, -0.0137,\n",
      "          0.0807,  0.1395],\n",
      "        [-0.0499, -0.1366,  0.0642,  0.1399, -0.1234, -0.1398,  0.1337, -0.0534,\n",
      "         -0.0297, -0.0136, -0.0396,  0.1346,  0.0466, -0.0507,  0.0020,  0.0346,\n",
      "         -0.0460,  0.0385,  0.1066, -0.1391, -0.0038, -0.1291,  0.0592, -0.0619,\n",
      "          0.0395,  0.1143, -0.0946,  0.1373, -0.0516,  0.0216, -0.0041, -0.0518,\n",
      "          0.1228, -0.0206, -0.0033,  0.0695,  0.0763,  0.0715, -0.1126, -0.0901,\n",
      "         -0.0904, -0.0202, -0.0204,  0.1096, -0.1390, -0.1342, -0.1023, -0.0379,\n",
      "          0.0310,  0.0666],\n",
      "        [-0.0587, -0.0077,  0.1259,  0.0668,  0.0294, -0.0272,  0.1267, -0.0720,\n",
      "          0.0588, -0.0592,  0.1090, -0.1247, -0.1149,  0.0124, -0.0973, -0.1157,\n",
      "          0.0927, -0.1338, -0.1223,  0.1154, -0.0623,  0.0892, -0.1056, -0.0971,\n",
      "         -0.1025,  0.0314,  0.0110, -0.0047, -0.0246, -0.0109, -0.1287, -0.0971,\n",
      "          0.0456, -0.1022, -0.0801, -0.1291,  0.0299, -0.1270,  0.0407, -0.0691,\n",
      "          0.1311, -0.1229, -0.0145, -0.1377, -0.0022,  0.0392,  0.0779,  0.0128,\n",
      "         -0.1031,  0.0723],\n",
      "        [ 0.1289, -0.1247,  0.0216,  0.1410, -0.1143,  0.0891, -0.1256, -0.0452,\n",
      "         -0.1008, -0.0648,  0.1306,  0.0065, -0.0859, -0.0942,  0.1072,  0.0110,\n",
      "         -0.0831, -0.0969, -0.0065, -0.1221, -0.0838, -0.0337,  0.0470, -0.0467,\n",
      "          0.1084,  0.0357,  0.0395,  0.0723,  0.1004, -0.1168, -0.0858, -0.1347,\n",
      "         -0.0115, -0.0470, -0.0719, -0.0785,  0.0416,  0.0990, -0.0539, -0.0924,\n",
      "         -0.0278,  0.0448,  0.1311,  0.0690, -0.0793, -0.0532, -0.1341,  0.0065,\n",
      "         -0.1228, -0.1180],\n",
      "        [ 0.0633,  0.0527, -0.1274, -0.0762, -0.0701, -0.0884, -0.1402,  0.0387,\n",
      "         -0.0923, -0.0381, -0.0256, -0.0207,  0.1068, -0.0266, -0.0640,  0.1075,\n",
      "          0.1275, -0.0577, -0.0472,  0.0593, -0.0964, -0.0083, -0.0549,  0.0406,\n",
      "          0.0565, -0.0720,  0.0204,  0.0742, -0.0122,  0.0678,  0.0083, -0.0977,\n",
      "         -0.1307,  0.0745, -0.0337,  0.1325,  0.0708,  0.0494,  0.1356, -0.0488,\n",
      "          0.1060,  0.0996,  0.0946, -0.0520, -0.0717, -0.0657,  0.0073, -0.0426,\n",
      "         -0.0352,  0.0159],\n",
      "        [-0.1037,  0.0518, -0.1164,  0.0936, -0.0657,  0.0339, -0.0364, -0.0235,\n",
      "          0.1210,  0.0972,  0.0869,  0.0488,  0.0831, -0.0116,  0.0308, -0.0181,\n",
      "          0.1115, -0.0740, -0.0279,  0.0228,  0.0975,  0.0999,  0.0056, -0.0062,\n",
      "         -0.0098,  0.0327,  0.0024, -0.1346,  0.1167,  0.0332, -0.0240, -0.1148,\n",
      "          0.0211, -0.1214,  0.0456,  0.1204, -0.0095, -0.0680,  0.0642, -0.1080,\n",
      "          0.0297, -0.0425, -0.1073,  0.0107,  0.1075, -0.1321, -0.0911, -0.1019,\n",
      "         -0.0783, -0.1374],\n",
      "        [-0.1047, -0.1246, -0.1257,  0.1321,  0.0841,  0.0723,  0.1223,  0.0347,\n",
      "         -0.0166, -0.0476,  0.0434, -0.0535,  0.0303, -0.1178,  0.0459,  0.0673,\n",
      "         -0.1111, -0.0138,  0.1301,  0.1326,  0.0545,  0.1221, -0.1404, -0.1334,\n",
      "         -0.1011, -0.0560,  0.1379,  0.0710, -0.0770, -0.0551,  0.1258,  0.0338,\n",
      "         -0.0389, -0.0965, -0.0831, -0.0344,  0.0161,  0.0731, -0.1239, -0.0826,\n",
      "         -0.0574, -0.0189, -0.0127, -0.1249, -0.0168,  0.0825,  0.1013,  0.1200,\n",
      "          0.0838,  0.0022],\n",
      "        [ 0.0135,  0.0827,  0.0682, -0.0765, -0.0629,  0.0305,  0.0629,  0.0636,\n",
      "          0.1282, -0.1186, -0.1256,  0.1025,  0.0520, -0.0059,  0.1112, -0.0924,\n",
      "          0.1114, -0.1071, -0.0354,  0.1195, -0.0815, -0.0062,  0.1267,  0.0764,\n",
      "         -0.0326, -0.0525,  0.0589,  0.0914,  0.1410,  0.0911,  0.0952, -0.0049,\n",
      "         -0.0483,  0.0796, -0.0040, -0.0611,  0.1178,  0.1109, -0.1112,  0.0471,\n",
      "          0.1218,  0.0840, -0.1082, -0.0760, -0.0085, -0.1380, -0.1049,  0.0025,\n",
      "          0.0634, -0.0340]], requires_grad=True)\n",
      "encoders.0.l2mu.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0079, -0.0389, -0.0689, -0.0295, -0.0276,  0.0126, -0.1356,  0.1006,\n",
      "         0.1411, -0.1015], requires_grad=True)\n",
      "encoders.0.l2var.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1179, -0.0445,  0.0187,  0.0538, -0.0433,  0.0167, -0.0521,  0.1371,\n",
      "         -0.0023,  0.0619,  0.1380, -0.0200,  0.1143, -0.0941, -0.1350, -0.0507,\n",
      "         -0.0271,  0.0458, -0.0437, -0.1405,  0.0696,  0.0638, -0.0152,  0.0784,\n",
      "          0.0482, -0.0304, -0.0297, -0.0913, -0.0833,  0.1074,  0.0621,  0.1094,\n",
      "         -0.0443,  0.0228,  0.1102,  0.1036, -0.0253, -0.0606,  0.1297, -0.0828,\n",
      "          0.1032,  0.0827, -0.0962, -0.0190, -0.1058,  0.0825, -0.0154,  0.0936,\n",
      "         -0.0871,  0.1291],\n",
      "        [ 0.1375, -0.1004,  0.0091,  0.0489, -0.0494, -0.1247,  0.0089,  0.0258,\n",
      "          0.1030, -0.0760, -0.0751,  0.0223, -0.1303,  0.0048, -0.0331,  0.0802,\n",
      "          0.0390,  0.0104,  0.0157, -0.0040, -0.0475, -0.0436,  0.0930,  0.0762,\n",
      "          0.0414,  0.1082, -0.0009,  0.0669,  0.1201, -0.0442, -0.0829,  0.0160,\n",
      "          0.0087, -0.0688, -0.1229,  0.1268,  0.0700,  0.1206,  0.0211, -0.1167,\n",
      "         -0.0037,  0.0988, -0.0563, -0.0602,  0.0573, -0.0939,  0.0075, -0.0790,\n",
      "         -0.1225, -0.0689],\n",
      "        [-0.1295,  0.0412,  0.0979,  0.0257, -0.0279, -0.1317, -0.0284, -0.0507,\n",
      "          0.0376,  0.0652,  0.1116,  0.0764, -0.0305,  0.0055,  0.1385,  0.1067,\n",
      "          0.1302, -0.1303, -0.0127, -0.0672,  0.0766, -0.0671,  0.1126,  0.1091,\n",
      "         -0.0985,  0.0317,  0.0692, -0.0099, -0.0289,  0.0633,  0.1278,  0.0795,\n",
      "         -0.0192,  0.0539, -0.0610, -0.0748, -0.1335,  0.1346, -0.1230,  0.1086,\n",
      "         -0.0939, -0.1069,  0.1021,  0.1346, -0.0353, -0.0132, -0.0103,  0.0404,\n",
      "          0.0890, -0.0977],\n",
      "        [ 0.0930, -0.0559, -0.0844, -0.0690, -0.0311, -0.1205, -0.0820,  0.1043,\n",
      "          0.0347, -0.1118,  0.0761,  0.1314,  0.0228,  0.1226, -0.0727, -0.0592,\n",
      "         -0.0774,  0.1156,  0.1321, -0.1085, -0.1216,  0.1244,  0.0996, -0.0837,\n",
      "          0.0122,  0.0838,  0.0516,  0.0460, -0.0836, -0.1268,  0.0743,  0.0535,\n",
      "          0.0358, -0.1412, -0.0460,  0.0625, -0.0477, -0.0928, -0.0112,  0.1135,\n",
      "         -0.0042,  0.1018, -0.0786, -0.0658, -0.0578,  0.1226, -0.0284,  0.0232,\n",
      "         -0.0223, -0.1036],\n",
      "        [ 0.1305, -0.0901,  0.0344, -0.0974, -0.0294, -0.0402, -0.0905,  0.1354,\n",
      "         -0.0895, -0.0191,  0.0125, -0.0673, -0.0384, -0.0739, -0.1195,  0.0041,\n",
      "          0.1394, -0.0826,  0.1218,  0.0326,  0.0941, -0.0032,  0.0842, -0.0595,\n",
      "         -0.1121,  0.1187,  0.0443, -0.0077,  0.1088, -0.0569,  0.0576,  0.0539,\n",
      "         -0.1380,  0.0249,  0.0357, -0.1060,  0.0427, -0.0012, -0.1362,  0.1414,\n",
      "         -0.0723, -0.0163,  0.1021, -0.0745, -0.1397, -0.0339, -0.1188, -0.1398,\n",
      "          0.0278, -0.0813],\n",
      "        [-0.0662, -0.0611,  0.0432,  0.0008,  0.1291,  0.0287,  0.0263,  0.0486,\n",
      "         -0.0619,  0.0739, -0.0002, -0.0861, -0.0321,  0.1260,  0.0722,  0.0490,\n",
      "          0.0892,  0.0059,  0.0372,  0.1103, -0.0602,  0.1380, -0.0687, -0.1199,\n",
      "          0.0091, -0.1057,  0.0467,  0.0313, -0.0204, -0.0198, -0.0304,  0.0261,\n",
      "          0.0860, -0.0244, -0.0132,  0.1247,  0.1375, -0.0208,  0.1027, -0.0964,\n",
      "         -0.0523,  0.0039,  0.0256,  0.0063, -0.1094,  0.1310,  0.0457, -0.0634,\n",
      "         -0.0018, -0.1279],\n",
      "        [-0.0849,  0.1198,  0.0271,  0.0047,  0.0366,  0.0208,  0.1137, -0.0588,\n",
      "         -0.1339, -0.0801,  0.0003, -0.1337,  0.0195,  0.0073, -0.1376, -0.0849,\n",
      "          0.0931, -0.0096,  0.0639,  0.0536,  0.0456,  0.1076, -0.0885, -0.0661,\n",
      "         -0.0042,  0.0886, -0.0115,  0.1161, -0.1198, -0.0370,  0.0472, -0.0436,\n",
      "          0.0214, -0.0714, -0.1231, -0.0092,  0.0674, -0.0065,  0.1391,  0.0330,\n",
      "          0.0168,  0.0031,  0.0783, -0.1368,  0.1275, -0.1320,  0.1006,  0.0678,\n",
      "         -0.1167, -0.0970],\n",
      "        [ 0.0039,  0.0830,  0.0535,  0.0537, -0.0568,  0.0485, -0.0537,  0.0641,\n",
      "         -0.0964,  0.0841,  0.0862, -0.0451,  0.0475,  0.0686,  0.1406, -0.1107,\n",
      "          0.0350, -0.0043,  0.0020, -0.0793,  0.0956,  0.0097,  0.0507, -0.0572,\n",
      "         -0.1215,  0.0793,  0.0101, -0.0662,  0.0639,  0.1275,  0.0209,  0.1206,\n",
      "          0.1060, -0.0315, -0.0942,  0.1033, -0.0208, -0.0088,  0.1412, -0.0882,\n",
      "          0.1293,  0.0909, -0.0738,  0.0293, -0.0475, -0.0032,  0.0892,  0.0419,\n",
      "         -0.0875,  0.1313],\n",
      "        [-0.1090, -0.0779, -0.0625,  0.0610,  0.0779,  0.1224,  0.0037,  0.1126,\n",
      "         -0.0612, -0.0401, -0.0346,  0.0932, -0.1340, -0.0335, -0.0807,  0.1215,\n",
      "         -0.1354, -0.1250, -0.1009, -0.1188,  0.0952,  0.0261, -0.0602,  0.0487,\n",
      "          0.1304, -0.0773,  0.1351,  0.1064,  0.1156, -0.0730,  0.1031, -0.0191,\n",
      "          0.0748, -0.0020,  0.1336,  0.0796,  0.0238,  0.1013,  0.1025,  0.1303,\n",
      "         -0.0344, -0.0490, -0.0734, -0.0675, -0.0939, -0.1114,  0.0079, -0.0201,\n",
      "          0.1246, -0.0470],\n",
      "        [ 0.0496,  0.1092, -0.0034,  0.1380, -0.1410,  0.1034,  0.0202, -0.0081,\n",
      "          0.1203, -0.0022,  0.0230,  0.0500,  0.0794, -0.0040, -0.0813, -0.0157,\n",
      "          0.0420,  0.0856,  0.1055, -0.0269, -0.0769,  0.0894, -0.0900, -0.0445,\n",
      "         -0.1192,  0.0822, -0.1166,  0.0672,  0.0605, -0.0445,  0.0247,  0.0697,\n",
      "         -0.0938,  0.0261, -0.0523, -0.0604, -0.0952,  0.0261,  0.0092,  0.1006,\n",
      "         -0.0301,  0.0786, -0.0566, -0.0213, -0.1184,  0.0965,  0.0656, -0.0758,\n",
      "         -0.0340,  0.0491]], requires_grad=True)\n",
      "encoders.0.l2var.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.1256,  0.0718,  0.0680, -0.0941,  0.0912,  0.0404, -0.0788,  0.0336,\n",
      "         0.0966,  0.0029], requires_grad=True)\n",
      "encoders.1.l1.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.0560e-02,  1.2720e-02,  1.3639e-02,  ..., -1.5453e-02,\n",
      "            3.2876e-04, -2.2561e-03],\n",
      "          [-3.1398e-03, -1.7363e-02, -1.8919e-02,  ..., -1.1908e-04,\n",
      "            2.1593e-02, -3.3918e-03],\n",
      "          [ 1.3938e-02,  1.8728e-02, -1.4613e-02,  ..., -2.2621e-02,\n",
      "           -1.9396e-02,  9.4324e-03],\n",
      "          ...,\n",
      "          [ 1.3717e-02,  2.0925e-03,  1.9956e-02,  ...,  1.5639e-02,\n",
      "           -1.2849e-02, -9.9726e-03],\n",
      "          [-4.3313e-03,  2.2162e-02, -3.5067e-03,  ...,  1.3058e-02,\n",
      "            7.3315e-03,  9.2300e-03],\n",
      "          [-1.7864e-02, -7.7653e-03,  2.2933e-02,  ...,  3.6363e-03,\n",
      "           -8.7794e-03,  1.2530e-02]],\n",
      "\n",
      "         [[-2.2647e-02,  1.0849e-02, -1.9648e-02,  ...,  8.0370e-03,\n",
      "           -1.0669e-03, -1.1306e-02],\n",
      "          [ 1.7974e-02,  3.5839e-03, -7.1299e-03,  ..., -7.1215e-03,\n",
      "            4.9571e-04,  1.8838e-02],\n",
      "          [-2.8292e-03,  1.0505e-02, -8.2393e-03,  ..., -4.3798e-03,\n",
      "           -1.8774e-03,  6.3911e-03],\n",
      "          ...,\n",
      "          [ 3.3283e-03, -2.1880e-02,  2.9227e-03,  ..., -1.9546e-02,\n",
      "           -1.2389e-02, -8.4456e-03],\n",
      "          [ 1.8889e-02,  1.5649e-02, -2.1603e-02,  ..., -1.5411e-02,\n",
      "           -1.2288e-02, -1.0842e-02],\n",
      "          [ 1.4677e-04, -2.1352e-03,  9.1560e-03,  ..., -1.4788e-02,\n",
      "           -1.1576e-02, -2.2833e-02]],\n",
      "\n",
      "         [[ 9.4904e-03,  3.6206e-03, -1.9731e-03,  ...,  9.2084e-03,\n",
      "           -2.1669e-02,  2.0156e-02],\n",
      "          [-7.5850e-03,  1.1419e-02,  2.2324e-02,  ..., -2.0386e-03,\n",
      "           -1.5708e-02,  2.2845e-02],\n",
      "          [ 8.1737e-03,  2.2341e-02, -8.9883e-03,  ..., -1.5777e-02,\n",
      "            3.7042e-04, -1.4130e-02],\n",
      "          ...,\n",
      "          [ 9.1001e-03,  1.3519e-02,  3.0095e-03,  ...,  6.4991e-03,\n",
      "            8.8009e-03,  3.3676e-03],\n",
      "          [ 1.2621e-02,  1.8854e-02, -1.3303e-02,  ...,  1.0778e-02,\n",
      "           -1.5386e-02,  1.6474e-02],\n",
      "          [-2.0781e-02,  1.5225e-02, -2.2498e-02,  ...,  1.2677e-02,\n",
      "            1.9661e-02,  1.3575e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5880e-02, -6.8368e-03,  2.8681e-03,  ...,  3.0265e-03,\n",
      "            2.1709e-02, -8.3435e-03],\n",
      "          [-7.9334e-03, -1.3374e-02,  4.6491e-03,  ..., -1.7531e-02,\n",
      "            2.0096e-02,  1.1202e-02],\n",
      "          [-2.1809e-02,  1.0759e-02, -2.7168e-03,  ...,  1.0630e-02,\n",
      "            7.9823e-03,  1.3703e-02],\n",
      "          ...,\n",
      "          [-8.0174e-03,  1.5222e-02,  2.0185e-02,  ...,  1.9104e-02,\n",
      "           -1.9681e-02,  1.1315e-02],\n",
      "          [ 1.8426e-02,  3.8051e-03,  1.3795e-02,  ...,  1.1455e-02,\n",
      "            8.3242e-03, -6.1714e-03],\n",
      "          [ 4.8328e-03, -1.3400e-02,  6.3575e-03,  ...,  1.1224e-02,\n",
      "           -9.8779e-03,  1.0514e-02]],\n",
      "\n",
      "         [[-1.8073e-04,  3.7221e-03,  1.9439e-02,  ..., -1.3468e-02,\n",
      "            1.8037e-02, -7.8046e-03],\n",
      "          [-1.5800e-02,  2.1977e-04, -1.3279e-02,  ..., -1.5427e-02,\n",
      "           -2.0602e-02, -8.9894e-03],\n",
      "          [ 1.6408e-02, -1.0108e-02, -4.1580e-03,  ...,  1.5559e-02,\n",
      "            4.2666e-03,  7.0553e-03],\n",
      "          ...,\n",
      "          [ 9.0139e-03, -6.3567e-06,  8.5628e-03,  ...,  1.4277e-02,\n",
      "           -7.6180e-03,  2.0738e-02],\n",
      "          [ 1.5062e-03,  2.9713e-03,  7.6743e-04,  ...,  1.9219e-02,\n",
      "           -1.1956e-03,  7.5637e-03],\n",
      "          [ 7.9573e-03,  1.3896e-02,  1.3721e-02,  ..., -3.1780e-03,\n",
      "            2.1819e-02, -2.5802e-03]],\n",
      "\n",
      "         [[ 2.0201e-03,  2.2008e-02,  4.2687e-03,  ...,  2.0015e-02,\n",
      "            2.7449e-03,  9.0629e-03],\n",
      "          [ 2.2555e-02, -1.9009e-02,  2.3036e-02,  ..., -1.4632e-02,\n",
      "            9.0086e-03,  1.0928e-02],\n",
      "          [ 1.4459e-02,  3.3989e-03,  2.2311e-02,  ..., -1.3079e-02,\n",
      "           -1.6295e-03, -2.2088e-02],\n",
      "          ...,\n",
      "          [ 2.0273e-02,  1.1721e-02, -1.9073e-02,  ...,  5.3478e-03,\n",
      "           -1.9882e-02,  2.1308e-02],\n",
      "          [-3.6270e-03, -1.1829e-02, -2.2812e-02,  ...,  8.8927e-03,\n",
      "            1.7734e-02, -5.3697e-03],\n",
      "          [ 1.0957e-02, -6.4025e-03, -1.4309e-02,  ...,  1.9145e-02,\n",
      "            2.4143e-03,  3.1542e-03]]]], requires_grad=True)\n",
      "encoders.1.l1.bias\n",
      "Parameter containing:\n",
      "tensor([0.0185, 0.0050], requires_grad=True)\n",
      "encoders.1.l2.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0021,  0.0076, -0.0158,  ..., -0.0199,  0.0016,  0.0252],\n",
      "          [-0.0087, -0.0271,  0.0072,  ..., -0.0066,  0.0182,  0.0157],\n",
      "          [ 0.0149,  0.0030, -0.0014,  ..., -0.0211,  0.0078, -0.0062],\n",
      "          ...,\n",
      "          [ 0.0175, -0.0155, -0.0190,  ..., -0.0239, -0.0097, -0.0044],\n",
      "          [-0.0243,  0.0151,  0.0276,  ..., -0.0243,  0.0244, -0.0275],\n",
      "          [ 0.0189,  0.0167, -0.0115,  ...,  0.0033, -0.0125,  0.0131]],\n",
      "\n",
      "         [[-0.0152,  0.0020,  0.0072,  ..., -0.0025,  0.0017,  0.0223],\n",
      "          [ 0.0271, -0.0191,  0.0094,  ...,  0.0013, -0.0171, -0.0032],\n",
      "          [ 0.0052, -0.0236,  0.0167,  ...,  0.0227, -0.0152,  0.0253],\n",
      "          ...,\n",
      "          [-0.0058, -0.0101,  0.0192,  ..., -0.0066, -0.0100,  0.0033],\n",
      "          [ 0.0108, -0.0137, -0.0137,  ..., -0.0059,  0.0265, -0.0131],\n",
      "          [ 0.0196,  0.0116,  0.0068,  ...,  0.0113, -0.0242,  0.0049]]]],\n",
      "       requires_grad=True)\n",
      "encoders.1.l2.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0192], requires_grad=True)\n",
      "encoders.1.l2mu.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0254,  0.0113,  0.0025, -0.0605,  0.0068,  0.0332, -0.0515,\n",
      "            0.0626,  0.0554,  0.0335, -0.0159,  0.0175,  0.0455],\n",
      "          [ 0.0564, -0.0394, -0.0553, -0.0509,  0.0482, -0.0107,  0.0387,\n",
      "            0.0063, -0.0309, -0.0766,  0.0769,  0.0590, -0.0580],\n",
      "          [ 0.0074,  0.0510, -0.0275, -0.0292,  0.0171, -0.0645,  0.0415,\n",
      "           -0.0615,  0.0353,  0.0758, -0.0378,  0.0065,  0.0316],\n",
      "          [ 0.0747, -0.0086, -0.0769, -0.0381, -0.0321,  0.0368,  0.0160,\n",
      "            0.0219, -0.0291, -0.0169,  0.0375, -0.0304,  0.0462],\n",
      "          [-0.0373,  0.0327, -0.0297, -0.0183,  0.0741,  0.0768,  0.0321,\n",
      "           -0.0590, -0.0339,  0.0046, -0.0171, -0.0738,  0.0321],\n",
      "          [-0.0453,  0.0305,  0.0241,  0.0180,  0.0045,  0.0145, -0.0590,\n",
      "            0.0756,  0.0501, -0.0653,  0.0074,  0.0445,  0.0209],\n",
      "          [ 0.0054, -0.0160,  0.0583,  0.0251, -0.0495, -0.0295,  0.0576,\n",
      "            0.0483, -0.0114,  0.0523,  0.0257,  0.0399, -0.0095],\n",
      "          [ 0.0716, -0.0192,  0.0608, -0.0370, -0.0107,  0.0430, -0.0459,\n",
      "           -0.0401, -0.0520,  0.0481, -0.0133, -0.0084,  0.0699],\n",
      "          [-0.0049,  0.0065, -0.0353, -0.0405,  0.0040,  0.0233, -0.0640,\n",
      "            0.0007, -0.0618,  0.0349,  0.0571, -0.0689, -0.0535],\n",
      "          [-0.0201, -0.0211,  0.0477,  0.0040, -0.0605,  0.0409,  0.0522,\n",
      "           -0.0336, -0.0718,  0.0507,  0.0516,  0.0159, -0.0682],\n",
      "          [-0.0558, -0.0316,  0.0048,  0.0343, -0.0588,  0.0128,  0.0118,\n",
      "            0.0700, -0.0560,  0.0099,  0.0043, -0.0741, -0.0659],\n",
      "          [ 0.0109, -0.0691, -0.0769, -0.0689, -0.0457, -0.0648, -0.0184,\n",
      "           -0.0751, -0.0353, -0.0683, -0.0021,  0.0698,  0.0037],\n",
      "          [ 0.0621,  0.0554, -0.0029, -0.0049, -0.0411, -0.0746,  0.0728,\n",
      "            0.0162, -0.0065,  0.0489, -0.0549,  0.0340, -0.0447]]]],\n",
      "       requires_grad=True)\n",
      "encoders.1.l2mu.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0753], requires_grad=True)\n",
      "encoders.1.l2var.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0291, -0.0701,  0.0012, -0.0377, -0.0753,  0.0469,  0.0657,\n",
      "           -0.0313,  0.0618, -0.0494, -0.0553, -0.0317,  0.0108],\n",
      "          [-0.0584, -0.0030,  0.0012, -0.0210,  0.0069,  0.0443, -0.0346,\n",
      "            0.0285, -0.0136, -0.0665, -0.0593,  0.0264,  0.0763],\n",
      "          [ 0.0379, -0.0705,  0.0135, -0.0664,  0.0625,  0.0142,  0.0104,\n",
      "            0.0583,  0.0620, -0.0254,  0.0455, -0.0122,  0.0237],\n",
      "          [ 0.0733, -0.0623,  0.0451, -0.0004,  0.0396,  0.0698, -0.0146,\n",
      "           -0.0520,  0.0190,  0.0012,  0.0458, -0.0248,  0.0656],\n",
      "          [-0.0503,  0.0248,  0.0185, -0.0664,  0.0573, -0.0340,  0.0123,\n",
      "           -0.0590, -0.0729,  0.0473, -0.0682, -0.0002, -0.0245],\n",
      "          [-0.0636, -0.0018, -0.0162, -0.0454,  0.0173,  0.0089,  0.0225,\n",
      "            0.0135,  0.0359, -0.0297, -0.0169, -0.0016,  0.0511],\n",
      "          [-0.0462,  0.0528,  0.0585,  0.0184,  0.0240, -0.0167,  0.0474,\n",
      "           -0.0500,  0.0125,  0.0280,  0.0047,  0.0575,  0.0163],\n",
      "          [-0.0202, -0.0434, -0.0721,  0.0331, -0.0663,  0.0008, -0.0488,\n",
      "           -0.0591,  0.0421,  0.0070,  0.0589, -0.0003,  0.0051],\n",
      "          [ 0.0004,  0.0273, -0.0337,  0.0052, -0.0321,  0.0057, -0.0225,\n",
      "            0.0236,  0.0626, -0.0161, -0.0181, -0.0161,  0.0685],\n",
      "          [ 0.0402,  0.0535, -0.0686, -0.0303, -0.0166,  0.0177, -0.0617,\n",
      "            0.0357,  0.0404,  0.0675, -0.0320, -0.0182,  0.0690],\n",
      "          [ 0.0531, -0.0171,  0.0426, -0.0475, -0.0679,  0.0367, -0.0437,\n",
      "           -0.0387, -0.0005, -0.0635,  0.0292,  0.0615,  0.0658],\n",
      "          [-0.0066, -0.0342,  0.0179,  0.0680, -0.0634,  0.0588,  0.0121,\n",
      "           -0.0311, -0.0618,  0.0637, -0.0061,  0.0316, -0.0230],\n",
      "          [ 0.0139,  0.0763, -0.0636, -0.0233,  0.0164,  0.0449, -0.0607,\n",
      "            0.0219,  0.0639,  0.0477, -0.0321,  0.0421, -0.0013]]]],\n",
      "       requires_grad=True)\n",
      "encoders.1.l2var.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0664], requires_grad=True)\n",
      "decoders.0.0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 2.8883e-01, -2.4740e-01, -3.1131e-01,  2.9036e-01, -9.6867e-02,\n",
      "          1.4484e-01, -2.9010e-02, -8.7138e-02, -1.4601e-01,  1.8964e-01],\n",
      "        [-1.2744e-01, -1.9405e-01, -3.4677e-02, -3.0349e-01,  1.2505e-01,\n",
      "         -3.1615e-01,  2.9927e-01,  2.0590e-01, -1.7520e-01,  7.0870e-03],\n",
      "        [ 6.7791e-02,  3.7131e-02,  1.9103e-01,  2.1476e-01, -2.7403e-01,\n",
      "          9.0935e-02,  1.9510e-01,  1.1088e-01,  1.0384e-01, -6.1103e-03],\n",
      "        [ 2.6649e-01, -2.7661e-01,  3.6149e-02,  7.5379e-02, -1.9903e-01,\n",
      "          2.8404e-01,  2.8725e-01,  1.9779e-01,  2.7697e-01,  2.6895e-01],\n",
      "        [ 2.6906e-01, -1.1990e-02, -2.4904e-01, -2.1819e-01,  1.4030e-01,\n",
      "         -1.5027e-01, -7.0684e-02, -1.6785e-01, -3.0832e-01,  2.9425e-01],\n",
      "        [-2.0231e-01, -1.3622e-01, -2.3779e-01, -1.7806e-01, -1.1077e-01,\n",
      "         -1.8420e-01,  2.4349e-01, -1.0016e-01,  2.1063e-01, -1.3945e-01],\n",
      "        [-5.5615e-02, -6.1284e-02, -1.9809e-01,  1.3078e-01, -2.9570e-01,\n",
      "         -1.5679e-01,  2.8848e-01,  9.0244e-02, -5.9789e-02,  6.0235e-02],\n",
      "        [-2.5501e-01,  8.4516e-02,  2.5270e-01,  9.8084e-02,  1.3769e-01,\n",
      "         -1.6091e-01,  2.3621e-01,  3.1358e-01,  8.4426e-02,  4.4139e-02],\n",
      "        [ 1.4097e-01,  2.2776e-02, -1.4055e-01, -5.0733e-02, -2.2703e-01,\n",
      "          5.3378e-02, -1.2813e-01, -7.1058e-03, -1.3770e-01, -2.9782e-01],\n",
      "        [-2.6527e-01,  2.1292e-01,  3.1578e-01, -1.0567e-01, -2.3955e-01,\n",
      "          2.7058e-01, -2.2792e-01, -3.0587e-01, -6.7238e-02, -2.2959e-01],\n",
      "        [ 1.2517e-02, -1.6157e-01, -2.4663e-01,  5.0940e-02, -2.5218e-01,\n",
      "          3.1536e-01, -3.0899e-01, -3.0137e-01,  2.5067e-01,  2.8928e-01],\n",
      "        [ 1.3681e-01,  3.9018e-02,  4.4531e-03, -1.9275e-01, -2.2405e-01,\n",
      "          7.2243e-02,  1.4317e-01, -7.4363e-02, -1.2938e-01,  1.8012e-01],\n",
      "        [-2.6932e-01, -2.0587e-02,  1.9898e-01, -9.9425e-02, -5.9589e-02,\n",
      "          9.5403e-02,  2.5145e-02, -4.9248e-02, -1.8188e-01,  1.0441e-01],\n",
      "        [-3.7663e-02, -1.3002e-01,  2.4767e-01,  1.0465e-01, -1.8050e-01,\n",
      "          1.0075e-01, -3.0006e-01,  1.0793e-01,  1.9267e-01,  1.1203e-02],\n",
      "        [ 2.7360e-01,  1.0149e-01,  9.3976e-02, -2.4381e-01, -6.4338e-02,\n",
      "         -1.3314e-01, -9.5411e-02,  2.0981e-01, -5.2090e-02,  4.2515e-02],\n",
      "        [ 2.0451e-04, -2.2165e-01,  2.4213e-01,  2.8822e-01,  2.2044e-01,\n",
      "          2.6605e-01, -6.6356e-02,  2.9348e-01, -2.1460e-01, -9.6555e-02],\n",
      "        [ 6.3085e-02, -2.7440e-01, -1.4969e-02,  4.9794e-02,  2.8461e-01,\n",
      "          8.0451e-02,  1.6011e-01, -3.0905e-01, -3.1262e-01, -3.1540e-02],\n",
      "        [ 3.3620e-02, -3.0832e-01,  1.3239e-01, -1.9604e-02, -7.0807e-02,\n",
      "          2.3387e-01,  2.3538e-01,  2.3182e-02,  2.4232e-02, -2.6693e-01],\n",
      "        [-1.3963e-04,  1.4756e-01,  2.5351e-01, -4.9071e-02,  1.9326e-01,\n",
      "          2.2137e-01, -3.2286e-02,  1.1576e-01, -2.0791e-01,  1.1797e-01],\n",
      "        [ 2.0456e-01, -9.0945e-02,  8.3362e-03,  2.1486e-01, -1.6003e-01,\n",
      "          8.1902e-02,  2.6128e-01,  8.0581e-02,  6.0639e-03, -2.7872e-01],\n",
      "        [-2.0913e-01, -2.4809e-01,  1.3441e-01,  2.2337e-01, -1.2503e-01,\n",
      "         -8.7531e-03, -2.3075e-01, -1.6621e-01, -2.3810e-01,  9.4121e-02],\n",
      "        [-9.9621e-02, -5.4315e-02, -3.4328e-02, -1.3682e-01, -2.3644e-01,\n",
      "          1.0714e-01,  1.3282e-01,  1.3007e-01, -2.6550e-01, -6.0108e-02],\n",
      "        [ 7.6345e-02,  3.0053e-01,  3.0571e-01,  3.2985e-02, -3.1100e-03,\n",
      "          2.2194e-01, -2.8598e-01,  9.7426e-02, -1.9090e-01, -3.1437e-02],\n",
      "        [-8.4577e-02,  1.6525e-01, -2.9099e-01,  2.5154e-01, -2.6968e-01,\n",
      "          2.6749e-01,  1.5315e-01, -3.0045e-01,  1.7732e-01, -5.0206e-02],\n",
      "        [-1.7994e-01, -2.5163e-01,  7.5510e-02,  7.4607e-02, -4.9884e-02,\n",
      "          2.0924e-01, -8.0259e-02, -5.1465e-02,  2.3173e-01, -1.8190e-02],\n",
      "        [-2.3207e-01,  1.3186e-01,  1.4124e-01, -1.4542e-02,  1.4894e-01,\n",
      "          2.7852e-01,  7.9999e-02, -9.0957e-02, -1.2760e-01, -2.2929e-01],\n",
      "        [ 2.1789e-01,  1.2260e-01, -4.5943e-02, -1.3049e-01,  6.2954e-02,\n",
      "          3.1573e-01, -2.7634e-01, -2.0600e-01,  1.9528e-01, -1.9536e-01],\n",
      "        [ 6.0497e-02, -1.6882e-01,  2.8057e-01, -2.9587e-01, -2.6756e-01,\n",
      "          1.3223e-01,  3.2367e-02, -2.0328e-01,  1.8949e-01, -3.1614e-01],\n",
      "        [ 1.8554e-01, -1.3113e-01, -3.0543e-01, -1.2536e-01,  3.1473e-01,\n",
      "          2.2888e-01,  7.6917e-02, -1.9749e-01, -1.1664e-01,  2.8140e-01],\n",
      "        [-2.4896e-01, -1.7801e-01, -5.7892e-02,  5.1600e-02,  1.6405e-01,\n",
      "         -1.0488e-01,  1.7269e-01,  1.1300e-01, -1.4818e-01, -7.6414e-02],\n",
      "        [ 8.7078e-02,  1.0971e-01,  5.9545e-02, -1.7024e-01,  5.9032e-02,\n",
      "         -5.4968e-02,  2.5258e-01, -2.0617e-02, -2.1856e-01, -1.6693e-01],\n",
      "        [ 2.2653e-01, -1.8185e-02, -2.5017e-01, -2.4798e-01, -2.6901e-01,\n",
      "         -2.7190e-01,  6.9869e-02, -2.4985e-01,  6.7586e-02, -2.4066e-01],\n",
      "        [ 1.2567e-01,  1.8961e-01,  2.6940e-02, -2.6098e-01,  2.9982e-02,\n",
      "         -1.4722e-01,  2.2892e-01, -1.3744e-01,  2.8431e-01, -2.5560e-01],\n",
      "        [ 2.8390e-01, -2.7246e-01, -2.6034e-02, -9.7371e-03,  7.6468e-02,\n",
      "         -3.1371e-01, -1.7972e-01,  1.4996e-01, -1.9378e-01, -5.2296e-02],\n",
      "        [-8.1970e-02,  1.3851e-01, -2.1090e-01,  1.7864e-01,  3.1284e-01,\n",
      "         -1.0504e-02, -9.3868e-02,  1.1081e-01, -2.1993e-01, -1.9794e-01],\n",
      "        [ 1.1264e-02, -1.5647e-01, -6.3325e-02, -1.9403e-01,  7.6170e-02,\n",
      "          1.0015e-01,  2.6813e-01,  3.1536e-01,  2.0430e-01, -3.9962e-02],\n",
      "        [ 1.6847e-01,  1.1565e-01, -1.3190e-01,  2.4598e-01, -2.1168e-01,\n",
      "          1.9028e-01,  2.3865e-01, -2.3700e-01, -1.0356e-01, -1.5522e-02],\n",
      "        [ 1.2549e-04, -2.7289e-01,  1.5701e-01, -2.8911e-01,  2.1713e-01,\n",
      "          4.2678e-02, -1.8407e-01, -2.4692e-01,  2.1363e-01, -2.6816e-02],\n",
      "        [-2.8127e-01, -8.8434e-03, -2.5175e-01, -2.4153e-01,  4.3022e-02,\n",
      "          3.1508e-01, -1.0646e-01, -1.4843e-02, -2.2449e-01,  7.6895e-02],\n",
      "        [ 2.6269e-01, -1.8913e-01, -1.3882e-01, -5.6976e-04, -1.8161e-01,\n",
      "         -2.7722e-01, -2.1007e-01,  2.8159e-01,  1.6720e-01,  3.1191e-01],\n",
      "        [-9.1557e-02, -2.7240e-01,  2.3747e-01,  2.6535e-01,  1.4463e-01,\n",
      "          7.8715e-02,  1.0648e-01,  2.0692e-01,  2.6738e-01,  5.9590e-02],\n",
      "        [ 2.4033e-01,  3.1201e-02, -8.7905e-02,  2.5121e-02, -1.8299e-01,\n",
      "         -5.8611e-03, -3.9298e-02, -2.8081e-01, -4.7865e-02,  1.9549e-01],\n",
      "        [ 3.1075e-01, -1.2963e-01,  2.8764e-01, -8.2459e-02,  5.9233e-02,\n",
      "         -1.4676e-02, -4.0518e-02,  4.5055e-03,  2.9342e-01, -2.5525e-01],\n",
      "        [ 1.8933e-01, -6.7435e-02, -2.9919e-01,  2.3652e-02,  2.9515e-01,\n",
      "          1.6193e-01,  1.3794e-01, -3.1628e-02, -2.8984e-01, -1.4723e-01],\n",
      "        [ 2.1971e-01, -1.7055e-01, -3.1440e-01,  1.6944e-01,  1.4167e-01,\n",
      "          8.9512e-02,  2.9102e-01, -7.1281e-02,  6.3361e-03, -2.2074e-01],\n",
      "        [-3.0635e-01, -1.4458e-01, -4.3256e-02, -2.1006e-01,  1.4382e-01,\n",
      "          2.9244e-01, -1.0847e-01, -1.3526e-01, -2.1804e-02,  2.1650e-01],\n",
      "        [-6.7256e-02, -2.2604e-01, -1.9965e-01, -9.8811e-02,  1.0262e-01,\n",
      "         -2.0482e-01,  1.0122e-01,  9.5584e-02, -3.0247e-01, -7.5201e-02],\n",
      "        [ 6.9458e-02, -9.2925e-02, -7.1733e-02, -1.3274e-01, -4.2484e-02,\n",
      "          2.1160e-01, -1.4880e-01, -2.5988e-01, -1.4581e-01, -2.9822e-01],\n",
      "        [ 1.1341e-01, -2.9100e-01, -2.7575e-01, -1.7321e-01, -4.5655e-02,\n",
      "          2.3352e-02, -2.6197e-01,  1.9774e-01,  3.8753e-02, -2.3758e-01],\n",
      "        [-3.9453e-02,  1.5349e-01, -4.3186e-04,  2.2920e-01,  1.1969e-02,\n",
      "          4.5843e-02, -2.1296e-01,  1.8330e-01,  2.3843e-01,  1.2713e-01]],\n",
      "       requires_grad=True)\n",
      "decoders.0.0.bias\n",
      "Parameter containing:\n",
      "tensor([-0.2110,  0.1908,  0.1455,  0.0420,  0.0680,  0.2763,  0.0591, -0.2515,\n",
      "         0.0613,  0.2937, -0.1006,  0.0304, -0.0836,  0.2138,  0.1724,  0.3010,\n",
      "         0.2300, -0.0595,  0.1624,  0.1721, -0.2436, -0.2485, -0.0357,  0.0905,\n",
      "        -0.0140, -0.3026, -0.1426,  0.1678,  0.1924,  0.2094, -0.2525,  0.3037,\n",
      "         0.0121, -0.0442,  0.2592, -0.0424,  0.0020, -0.0317,  0.1102,  0.2285,\n",
      "         0.0798, -0.2615, -0.0341,  0.0056,  0.2487,  0.3084,  0.1851,  0.3043,\n",
      "        -0.1297, -0.0371], requires_grad=True)\n",
      "decoders.0.1.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0829,  0.0115,  0.0465,  ...,  0.0848,  0.0916, -0.1085],\n",
      "        [ 0.0195,  0.0800, -0.0164,  ...,  0.0252,  0.0886,  0.0549],\n",
      "        [ 0.0149,  0.0839,  0.0936,  ..., -0.0237,  0.0431,  0.0050],\n",
      "        ...,\n",
      "        [ 0.1197, -0.0008,  0.1164,  ...,  0.1047, -0.0470, -0.1360],\n",
      "        [ 0.0268, -0.0803, -0.0116,  ..., -0.0049,  0.0772,  0.1043],\n",
      "        [ 0.1052,  0.1153, -0.0641,  ...,  0.0419,  0.0474,  0.1221]],\n",
      "       requires_grad=True)\n",
      "decoders.0.1.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0095, -0.0492, -0.0654, -0.1325, -0.0337,  0.1093, -0.0283,  0.0452,\n",
      "        -0.0488,  0.0054, -0.1306, -0.0528,  0.1397,  0.0983, -0.0709, -0.1196,\n",
      "         0.1220,  0.0956,  0.0484, -0.0579,  0.0137,  0.0407, -0.1113, -0.0428,\n",
      "         0.1052, -0.0156, -0.0707, -0.0837, -0.0743, -0.1151,  0.0763, -0.0879,\n",
      "         0.1157, -0.0676, -0.0563,  0.0216,  0.1190, -0.0223, -0.0567, -0.0162,\n",
      "        -0.0830, -0.0177,  0.0843, -0.1079,  0.0990, -0.0359, -0.1168,  0.0922,\n",
      "        -0.1065, -0.0725,  0.0847,  0.0472, -0.0103, -0.1386, -0.0786,  0.0583,\n",
      "        -0.0535, -0.0735,  0.0689, -0.1162, -0.0917,  0.0125,  0.0015,  0.0047,\n",
      "        -0.0065,  0.1316,  0.1056, -0.0167,  0.0435,  0.0295, -0.0963,  0.0011,\n",
      "        -0.0011, -0.0930,  0.1343, -0.0069, -0.1124, -0.1269, -0.0271, -0.1161,\n",
      "         0.1396,  0.0388,  0.1065,  0.1136, -0.0706, -0.0086,  0.0465, -0.0074,\n",
      "        -0.0915,  0.0375,  0.0751, -0.0700,  0.1234,  0.0016, -0.0925, -0.0762,\n",
      "         0.0421,  0.0287,  0.1018,  0.0064], requires_grad=True)\n",
      "decoders.1.0.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0445,  0.0057, -0.0679,  0.0265, -0.0642, -0.0748,  0.0157,\n",
      "           -0.0220,  0.0649,  0.0363,  0.0085, -0.0120,  0.0649],\n",
      "          [-0.0501, -0.0346, -0.0524, -0.0210,  0.0671, -0.0299, -0.0250,\n",
      "            0.0435, -0.0538, -0.0208,  0.0512,  0.0472, -0.0594],\n",
      "          [ 0.0535,  0.0432,  0.0496, -0.0087, -0.0240, -0.0683, -0.0759,\n",
      "            0.0356, -0.0064,  0.0488, -0.0627, -0.0022,  0.0092],\n",
      "          [ 0.0337,  0.0278, -0.0632,  0.0636,  0.0521,  0.0337,  0.0733,\n",
      "            0.0044,  0.0611, -0.0521, -0.0766,  0.0761,  0.0018],\n",
      "          [-0.0256, -0.0495,  0.0228,  0.0387, -0.0629, -0.0645, -0.0121,\n",
      "           -0.0154,  0.0358, -0.0462, -0.0600,  0.0431, -0.0619],\n",
      "          [ 0.0492, -0.0275,  0.0713,  0.0639, -0.0499,  0.0681, -0.0388,\n",
      "           -0.0533, -0.0528, -0.0352, -0.0494, -0.0281,  0.0279],\n",
      "          [-0.0616,  0.0735, -0.0138,  0.0189,  0.0355,  0.0725,  0.0271,\n",
      "            0.0213, -0.0382,  0.0384,  0.0668, -0.0410,  0.0323],\n",
      "          [ 0.0232,  0.0695, -0.0511, -0.0012,  0.0099,  0.0084, -0.0471,\n",
      "           -0.0039,  0.0401, -0.0382, -0.0067, -0.0124, -0.0727],\n",
      "          [ 0.0082,  0.0154, -0.0586,  0.0067,  0.0120, -0.0727, -0.0084,\n",
      "           -0.0689, -0.0691,  0.0582,  0.0494,  0.0159,  0.0004],\n",
      "          [-0.0422,  0.0609, -0.0307,  0.0603, -0.0758, -0.0114, -0.0305,\n",
      "           -0.0057, -0.0441, -0.0510,  0.0075,  0.0391,  0.0739],\n",
      "          [-0.0294,  0.0525, -0.0039,  0.0494,  0.0715, -0.0709, -0.0500,\n",
      "           -0.0351, -0.0119, -0.0206, -0.0463, -0.0296,  0.0552],\n",
      "          [ 0.0164,  0.0572, -0.0261,  0.0473,  0.0461,  0.0562,  0.0303,\n",
      "           -0.0379, -0.0086, -0.0113, -0.0250,  0.0351, -0.0717],\n",
      "          [-0.0593,  0.0733, -0.0547, -0.0475, -0.0044,  0.0121,  0.0228,\n",
      "           -0.0284, -0.0545,  0.0476, -0.0192,  0.0329,  0.0579]]]],\n",
      "       requires_grad=True)\n",
      "decoders.1.0.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0393], requires_grad=True)\n",
      "decoders.1.1.weight\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0185, -0.0184,  0.0037,  ...,  0.0076, -0.0234,  0.0229],\n",
      "          [ 0.0195, -0.0211, -0.0172,  ...,  0.0148,  0.0222,  0.0173],\n",
      "          [ 0.0268, -0.0183, -0.0269,  ..., -0.0028,  0.0178, -0.0007],\n",
      "          ...,\n",
      "          [ 0.0201,  0.0131,  0.0130,  ...,  0.0110, -0.0017,  0.0134],\n",
      "          [-0.0179, -0.0173, -0.0101,  ...,  0.0014,  0.0263,  0.0197],\n",
      "          [-0.0198, -0.0214,  0.0212,  ...,  0.0028,  0.0275,  0.0097]],\n",
      "\n",
      "         [[-0.0231, -0.0234, -0.0212,  ..., -0.0014,  0.0200, -0.0165],\n",
      "          [ 0.0104, -0.0044,  0.0060,  ..., -0.0018,  0.0169, -0.0167],\n",
      "          [ 0.0060,  0.0093,  0.0112,  ..., -0.0170, -0.0004,  0.0214],\n",
      "          ...,\n",
      "          [-0.0189,  0.0059, -0.0140,  ..., -0.0131,  0.0269,  0.0191],\n",
      "          [ 0.0100,  0.0075,  0.0019,  ..., -0.0146,  0.0183,  0.0185],\n",
      "          [-0.0271, -0.0138,  0.0187,  ...,  0.0148, -0.0131,  0.0260]]]],\n",
      "       requires_grad=True)\n",
      "decoders.1.1.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0113, -0.0203], requires_grad=True)\n",
      "decoders.1.2.weight\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.9528e-02,  2.2711e-02,  9.5821e-03,  ..., -9.3676e-03,\n",
      "            7.7792e-03, -1.1967e-02],\n",
      "          [-1.4020e-02,  6.1719e-03,  2.9803e-03,  ...,  1.7336e-02,\n",
      "            1.8758e-02, -1.8274e-02],\n",
      "          [ 1.1916e-02,  1.8647e-02, -1.9881e-02,  ..., -2.1549e-02,\n",
      "            1.4460e-02, -1.4822e-02],\n",
      "          ...,\n",
      "          [-1.0020e-02,  1.7337e-02, -1.4876e-02,  ...,  1.2700e-02,\n",
      "            4.6671e-03, -3.3871e-03],\n",
      "          [-1.4628e-02, -6.7589e-03, -2.1285e-02,  ...,  1.4185e-02,\n",
      "            1.1709e-02, -2.5339e-03],\n",
      "          [ 8.4645e-04, -1.4296e-02,  1.1457e-02,  ...,  1.8181e-02,\n",
      "           -2.0930e-02, -8.9171e-03]],\n",
      "\n",
      "         [[-6.3796e-03,  7.4681e-03, -4.9769e-03,  ..., -4.7036e-03,\n",
      "            8.2421e-03, -1.8571e-02],\n",
      "          [-1.3894e-02,  1.1516e-02,  1.8737e-02,  ...,  1.7382e-02,\n",
      "            1.8724e-02,  7.1060e-03],\n",
      "          [ 2.0361e-03, -7.9451e-03, -8.4146e-04,  ..., -1.3568e-02,\n",
      "            2.1407e-03, -7.1383e-03],\n",
      "          ...,\n",
      "          [-6.8036e-03,  1.0850e-02, -4.5814e-03,  ..., -2.1232e-02,\n",
      "           -9.0570e-03,  8.0775e-03],\n",
      "          [-1.7880e-02,  1.8545e-02, -1.3496e-02,  ...,  1.5187e-02,\n",
      "            1.6951e-02, -8.4028e-03],\n",
      "          [-2.1082e-02,  1.0038e-02, -7.4193e-03,  ..., -5.8709e-03,\n",
      "            5.3011e-04,  2.0732e-02]],\n",
      "\n",
      "         [[ 5.6866e-04, -6.9506e-03,  1.3236e-02,  ...,  1.2552e-02,\n",
      "           -2.2564e-02,  1.9251e-02],\n",
      "          [ 4.3704e-03, -8.5520e-03, -1.7405e-02,  ...,  2.6294e-03,\n",
      "           -1.6916e-02, -1.3552e-02],\n",
      "          [ 1.0736e-02,  2.0011e-03, -3.3606e-03,  ...,  5.4161e-03,\n",
      "           -6.8846e-03, -1.6903e-02],\n",
      "          ...,\n",
      "          [-8.5347e-03,  2.0597e-02,  6.6350e-03,  ...,  9.7144e-03,\n",
      "            1.3556e-02, -6.5075e-03],\n",
      "          [-2.2684e-02,  3.3631e-03,  2.2278e-02,  ..., -8.2663e-03,\n",
      "            1.4107e-02, -2.8905e-03],\n",
      "          [ 7.2517e-03, -9.0984e-03, -3.3854e-03,  ...,  1.0158e-02,\n",
      "            1.4764e-02,  1.1340e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.0634e-02, -1.1976e-02,  1.2978e-02,  ...,  1.5852e-02,\n",
      "            2.1485e-02,  1.3982e-02],\n",
      "          [ 4.8692e-03,  1.3260e-02, -1.1795e-02,  ..., -2.2578e-02,\n",
      "           -8.0113e-05,  1.2429e-02],\n",
      "          [-4.9913e-03,  2.2264e-02, -1.6568e-02,  ..., -7.7374e-03,\n",
      "            7.8654e-03, -1.3532e-02],\n",
      "          ...,\n",
      "          [ 5.0277e-03, -2.1111e-02, -1.6376e-02,  ...,  1.2730e-02,\n",
      "           -6.5984e-03,  1.3894e-02],\n",
      "          [-1.8041e-03, -5.5956e-03, -1.9045e-02,  ...,  5.9097e-03,\n",
      "            1.8757e-03, -2.1529e-03],\n",
      "          [-1.7640e-02, -2.2504e-02, -2.1664e-02,  ...,  9.1874e-03,\n",
      "            2.2755e-02, -9.4689e-03]],\n",
      "\n",
      "         [[-2.1690e-02, -1.1289e-02,  1.8667e-02,  ...,  8.1979e-03,\n",
      "           -2.2477e-02,  1.8561e-02],\n",
      "          [-1.9278e-02, -7.2059e-03,  1.7680e-02,  ...,  1.5642e-02,\n",
      "            1.1195e-02, -6.2914e-03],\n",
      "          [-1.7710e-02, -1.4824e-02, -4.6058e-03,  ...,  2.2220e-02,\n",
      "           -1.2071e-02,  6.3457e-04],\n",
      "          ...,\n",
      "          [ 1.0877e-02,  4.7947e-04,  1.2669e-02,  ...,  1.4367e-02,\n",
      "           -1.0290e-02, -1.9578e-02],\n",
      "          [ 4.8113e-03, -4.6808e-03, -1.1037e-02,  ..., -7.2233e-03,\n",
      "           -2.2994e-02,  2.2671e-02],\n",
      "          [-9.3261e-03, -7.6012e-03, -7.1391e-03,  ...,  7.9854e-03,\n",
      "            1.1598e-02,  7.5168e-03]],\n",
      "\n",
      "         [[-5.1720e-03, -1.2863e-02,  1.3553e-02,  ...,  1.5414e-03,\n",
      "            3.7826e-03, -6.0927e-03],\n",
      "          [ 1.8509e-02, -1.0842e-02, -4.4215e-03,  ..., -7.2200e-03,\n",
      "           -1.2610e-02, -1.4075e-02],\n",
      "          [ 2.0253e-02,  3.7540e-03, -1.8465e-02,  ...,  2.6448e-04,\n",
      "           -1.9203e-02,  1.0156e-02],\n",
      "          ...,\n",
      "          [ 1.9601e-02,  1.6107e-02,  3.5542e-03,  ..., -8.4370e-03,\n",
      "           -1.5329e-02,  1.5573e-02],\n",
      "          [ 1.9172e-02, -1.8240e-03, -1.3526e-02,  ..., -2.2569e-02,\n",
      "            2.4343e-03, -1.8001e-03],\n",
      "          [-9.1875e-03, -7.6307e-03, -2.1492e-02,  ..., -7.9496e-04,\n",
      "           -1.8002e-02,  2.0337e-02]]]], requires_grad=True)\n",
      "decoders.1.2.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0211,  0.0109,  0.0089], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, para in net.named_parameters():\n",
    "    print(name)\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = torch.randn(10, 100, device=_device)\n",
    "data2 = torch.randn(10, 3, 64, 64, device=_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = net([data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
