{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from gibbs_sampler_poise.ipynb\n",
      "importing Jupyter notebook from kl_divergence_calculator.ipynb\n",
      "importing Jupyter notebook from data_preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import gibbs_sampler_poise\n",
    "import kl_divergence_calculator\n",
    "import data_preprocessing\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import umap\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# learning parameters\n",
    "latent_dim1 = 32\n",
    "latent_dim2 = 16\n",
    "batch_size = 10\n",
    "dim_MNIST   = 784\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tx = transforms.ToTensor()\n",
    "MNIST_TRAINING_PATH = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/training.pt\"\n",
    "SVHN_TRAINING_PATH  = \"/home/achint/Practice_code/VAE/SVHN/train_32x32.mat\"\n",
    "MNIST_TEST_PATH     = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/test.pt\"\n",
    "SVHN_TEST_PATH  = \"/home/achint/Practice_code/VAE/SVHN/test_32x32.mat\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "RECONSTRUCTION_PATH = \"/home/achint/Practice_code/Updated_POISE_VAE/MNIST_SVHN/reconstructions/\"\n",
    "PATH = \"/home/achint/Practice_code/Updated_POISE_VAE/MNIST_SVHN/mnist_svhn_parameters.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs directory and the reconstruction directory \n",
    "if os.path.exists(RECONSTRUCTION_PATH):\n",
    "    shutil.rmtree(RECONSTRUCTION_PATH)\n",
    "    os.makedirs(RECONSTRUCTION_PATH)\n",
    "\n",
    "if os.path.exists(SUMMARY_WRITER_PATH):\n",
    "    shutil.rmtree(SUMMARY_WRITER_PATH)\n",
    "    os.makedirs(SUMMARY_WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing MNIST and SVHN datasets\n",
    "joint_dataset_train=data_preprocessing.JointDataset(mnist_pt_path=MNIST_TRAINING_PATH,\n",
    "                             svhn_mat_path=SVHN_TRAINING_PATH)\n",
    "joint_dataset_test = data_preprocessing.JointDataset(mnist_pt_path=MNIST_TEST_PATH,\n",
    "                             svhn_mat_path=SVHN_TEST_PATH)\n",
    "\n",
    "joint_dataset_train_loader = DataLoader(\n",
    "    joint_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _latent_dims_type_setter(lds): # version 1.0\n",
    "    ret, ret_flatten = [], []\n",
    "    for ld in lds:\n",
    "        if hasattr(ld, '__iter__'): # Iterable\n",
    "            ld_tuple = tuple([i for i in ld])\n",
    "            if not all(map(lambda i: isinstance(i, int), ld_tuple)):\n",
    "                raise ValueError('`latent_dim` must be either iterable of ints or int.')\n",
    "            ret.append(ld_tuple)\n",
    "            ret_flatten.append(int(prod(ld_tuple)))\n",
    "        elif isinstance(ld, int):\n",
    "            ret.append((ld, ))\n",
    "            ret_flatten.append(ld)\n",
    "        else:\n",
    "            raise ValueError('`latent_dim` must be either iterable of ints or int.')\n",
    "    return ret, ret_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POISEVAE(nn.Module):\n",
    "    __version__ = 1.0\n",
    "    \n",
    "    def __init__(self, encoders, decoders, batch_size, latent_dims=None, use_mse_loss=True,\n",
    "                 device=_device):\n",
    "        \"\"\"\n",
    "        encoders: list of nn.Module\n",
    "            Each encoder must have an attribute `latent_dim` specifying the dimension of the\n",
    "            latent space to which it encodes. An alternative way to avoid adding this attribute\n",
    "            is to specify the `latent_dims` parameter (see below). \n",
    "            Note that each `latent_dim` must be unsqueezed, e.g. (10, ) is not the same as (10, 1).\n",
    "            \n",
    "        decoders: list of nn.Module\n",
    "            The number and indices of decoders must match those of encoders.\n",
    "            \n",
    "        batch_size: int\n",
    "        \n",
    "        latent_dims: iterable, optional; default None\n",
    "            The dimensions of the latent spaces to which the encoders encode. The indices of the \n",
    "            entries must match those of encoders. An alternative way to specify the dimensions is\n",
    "            to add the attribute `latent_dim` to each encoder (see above).\n",
    "            Note that each entry must be unsqueezed, e.g. (10, ) is not the same as (10, 1).\n",
    "        \n",
    "        use_mse_loss: boolean, optional; default True\n",
    "            To use MSE loss or not; if not, BCE loss will be used.\n",
    "        \n",
    "        device: torch.device, optional\n",
    "        \"\"\"\n",
    "        super(POISEVAE,self).__init__()\n",
    "\n",
    "        if len(encoders) != len(decoders):\n",
    "            raise ValueError('The number of encoders must match that of decoders.')\n",
    "        \n",
    "        if len(encoders) > 2:\n",
    "            raise NotImplementedError('> 3 latent spaces not yet supported.')\n",
    "        \n",
    "        # Type check\n",
    "        if not all(map(lambda x: isinstance(x, nn.Module), (*encoders, *decoders))):\n",
    "            raise TypeError('`encoders` and `decoders` must be lists of `nn.Module` class.')\n",
    "\n",
    "        # Get the latent dimensions\n",
    "        if latent_dims is not None:\n",
    "            if not hasattr(latent_dims, '__iter__'): # Iterable\n",
    "                raise TypeError('`latent_dims` must be iterable.')\n",
    "            self.latent_dims = latent_dims\n",
    "        else:\n",
    "            self.latent_dims = tuple(map(lambda l: l.latent_dim, encoders))\n",
    "        self.latent_dims, self.latent_dims_flatten = _latent_dims_type_setter(self.latent_dims)\n",
    "\n",
    "        self.encoders = encoders\n",
    "        self.decoders = decoders\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.gibbs = gibbs_sampler_poise.gibbs_sampler(self.latent_dims_flatten, batch_size)\n",
    "        self.kl_div = kl_divergence_calculator.kl_divergence(self.latent_dims_flatten, batch_size)\n",
    "\n",
    "        self.register_parameter(name='g11', \n",
    "                                param=nn.Parameter(torch.randn(*self.latent_dims_flatten, \n",
    "                                                               device=self.device)))\n",
    "        self.register_parameter(name='g22', \n",
    "                                param=nn.Parameter(torch.randn(*self.latent_dims_flatten, \n",
    "                                                               device=self.device)))\n",
    "        self.flag_initialize = 1\n",
    "\n",
    "    def _decoder_helper(self):\n",
    "        \"\"\"\n",
    "        Reshape samples drawn from each latent space, and decode with considering the loss function\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        for decoder, z, ld in zip(self.decoders, self.z_gibbs_posteriors, self.latent_dims):\n",
    "            z = z.view(self.batch_size, *ld) # Match the shape to the output\n",
    "            x_ = decoder(z)\n",
    "            if not self.use_mse_loss: # BCE instead\n",
    "                x_ = torch.sigmoid(x_)\n",
    "            ret.append(x_)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, var = [], []\n",
    "        for i, xi in enumerate(x):\n",
    "            _mu, _log_var = self.encoders[i].forward(xi)\n",
    "            mu.append(_mu.view(self.batch_size, -1))\n",
    "            var.append(-torch.exp(_log_var.view(self.batch_size, -1)))\n",
    "\n",
    "        g22 = -torch.exp(self.g22)\n",
    "\n",
    "        # Initializing gibbs sample\n",
    "        if self.flag_initialize == 1:\n",
    "            z_priors = self.gibbs.sample(self.g11, g22, n_iterations=5000)\n",
    "            z_posteriors = self.gibbs.sample(self.g11, g22, lambda1s=mu, lambda2s=var,\n",
    "                                             n_iterations=5000)\n",
    "\n",
    "            self.z_priors = z_priors\n",
    "            self.z_posteriors = z_posteriors\n",
    "            self.flag_initialize = 0\n",
    "\n",
    "        z_priors = list(map(lambda z: z.detach(), self.z_priors))\n",
    "        z_posteriors = list(map(lambda z: z.detach(), self.z_posteriors))\n",
    "\n",
    "        # If lambda not provided, treat as zeros to save memory and computation\n",
    "        self.z_gibbs_priors = self.gibbs.sample(self.g11, g22, z=z_priors, n_iterations=5)\n",
    "        self.z_gibbs_posteriors = self.gibbs.sample(self.g11, g22, lambda1s=mu, lambda2s=var,\n",
    "                                                    z=z_posteriors, n_iterations=5)\n",
    "\n",
    "        self.z_priors = list(map(lambda z: z.detach(), self.z_gibbs_priors))\n",
    "        self.z_posteriors = list(map(lambda z: z.detach(), self.z_gibbs_posteriors))\n",
    "\n",
    "        G = torch.block_diag(self.g11, self.g22)\n",
    "\n",
    "        x_ = self._decoder_helper() # Decoding\n",
    "\n",
    "        # self.z2_gibbs_posterior = self.z2_gibbs_posterior.squeeze()\n",
    "        for i in range(len(self.z_gibbs_posteriors)):\n",
    "            self.z_gibbs_posteriors[i] = self.z_gibbs_posteriors[i].squeeze()\n",
    "\n",
    "        # KL loss\n",
    "        kls = self.kl_div.calc(G, self.z_gibbs_posteriors, self.z_gibbs_priors, mu,var)\n",
    "        KL_loss  = sum(kls)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        loss_func = nn.MSELoss(reduction='sum') if self.use_mse_loss else nn.BCELoss(reduction='sum')\n",
    "        recs = list(map(lambda x: loss_func(x[0], x[1]), zip(x_, x)))\n",
    "        rec_loss = sum(recs)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = KL_loss + rec_loss\n",
    "\n",
    "        return self.z_posteriors, x_, mu, var, loss, recs, KL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.latent_dim = 32\n",
    "        self.dim_MNIST   = 784\n",
    "\n",
    "        ## Encoder set1(MNIST)\n",
    "        self.set1_enc1 = nn.Linear(in_features = self.dim_MNIST,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*self.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Modality 1 (MNIST)\n",
    "        x       = F.relu(self.set1_enc1(x))\n",
    "        x       = F.relu(self.set1_enc2(x))  \n",
    "        x       = self.set1_enc3(x).view(-1,2,self.latent_dim)  # ->[128,2,32]\n",
    "        mu      = x[:,0,:] # ->[128,32]\n",
    "        log_var = x[:,1,:] # ->[128,32]\n",
    "        var     = -torch.exp(log_var)           #lambdap_2<0\n",
    "        return mu, log_var\n",
    "    \n",
    "class Encoder2(nn.Module):\n",
    "    # 64*64 -> 40*40 -> 16*16 -> 4*4\n",
    "    def __init__(self):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.latent_dim = 16     \n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set2_enc1 = nn.Conv2d(in_channels=3, out_channels=2*self.latent_dim, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_enc2 = nn.Conv2d(in_channels=2*self.latent_dim, out_channels=2*self.latent_dim, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_enc3 = nn.Conv2d(in_channels=2*self.latent_dim, out_channels=self.latent_dim, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4   \n",
    "        self.SVHNc1 = nn.Conv2d(self.latent_dim, self.latent_dim, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHNc2 = nn.Conv2d(self.latent_dim, self.latent_dim, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "    def forward(self, x):\n",
    "        # Modality 2 (SVHN)\n",
    "        x = x.view(-1,3, 32,32) \n",
    "        x = F.relu(self.set2_enc1(x))\n",
    "        x = F.relu(self.set2_enc2(x))\n",
    "        x = F.relu(self.set2_enc3(x))\n",
    "        # get 'mu' and 'log_var' for SVHN\n",
    "        mu = (self.SVHNc1(x).squeeze(3)).squeeze(2)\n",
    "        log_var = (self.SVHNc2(x).squeeze(3)).squeeze(2)\n",
    "        return mu, log_var\n",
    "    \n",
    "class Decoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder1, self).__init__()  \n",
    "        self.latent_dim = 32\n",
    "        self.dim_MNIST   = 784\n",
    "        ## Decoder set1(MNIST)\n",
    "        self.set1_dec1 = nn.Linear(in_features = self.latent_dim,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = self.dim_MNIST)\n",
    "\n",
    "class Decoder2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder2, self).__init__()  \n",
    "        self.latent_dim = 16    \n",
    "        ## Decoder set2(SVHN)\n",
    "        # input size: 16x1x1\n",
    "        self.set2_dec0 = nn.ConvTranspose2d(in_channels=self.latent_dim,out_channels=self.latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "        # input size: 16x4x4\n",
    "        self.set2_dec1 = nn.ConvTranspose2d(in_channels=self.latent_dim,out_channels=2*self.latent_dim, kernel_size=3, stride=1, padding=1)\n",
    "        # size: 32 x 4 x 4\n",
    "        self.set2_dec2 = nn.ConvTranspose2d(in_channels=2*self.latent_dim,out_channels=2*self.latent_dim, kernel_size=5, stride=1, padding=0)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_dec3 = nn.ConvTranspose2d(in_channels=2*self.latent_dim,out_channels=2*self.latent_dim, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_dec4 = nn.ConvTranspose2d(in_channels=2*self.latent_dim,out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 3 x 32 x 32\n",
    "        \n",
    "enc1 = Encoder1()\n",
    "enc2 = Encoder2().to(_device)\n",
    "dec1 = Decoder1().to(_device)\n",
    "dec2 = Decoder2().to(_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g11\n",
      "g22\n"
     ]
    }
   ],
   "source": [
    "#state = torch.load(PATH)\n",
    "model = POISEVAE([enc1, enc2], [dec1, dec2], batch_size,use_mse_loss=True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "#model.load_state_dict(state['state_dict'])\n",
    "#optimizer.load_state_dict(state['optimizer'])\n",
    "for name, para in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,joint_dataloader,epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i,joint_data in enumerate(joint_dataloader):\n",
    "        data1    = joint_data[0]\n",
    "        data1    = data1.float()\n",
    "        data2   = joint_data[1]\n",
    "        data2   = data2.float()\n",
    "        data1    = data1.to(device)\n",
    "        data2   = data2.to(device)\n",
    "        data1    = data1.view(data1.size(0), -1)\n",
    "        data2   = data2.view(data2.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "#        z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD       = model(data1,data2) \n",
    "        z_posterior,reconstruction,mu,var,loss,  KLD       = model(data1,data2) \n",
    "        running_mse1 += MSE1.item()\n",
    "        running_mse2 += MSE2.item()\n",
    "        running_kld  += KLD.item()\n",
    "        running_loss += loss.item()          #.item converts tensor with one element to number\n",
    "        loss.backward()                      #.backward\n",
    "        optimizer.step()                     #.step one learning step\n",
    "    train_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "    writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"training/KLD\", kld_loss, epoch)    \n",
    "    return train_loss\n",
    "    \n",
    "def test(model,joint_dataloader,epoch):\n",
    "    latent_repMNIST= []\n",
    "    latent_repSVHN= []\n",
    "    label_mnist= []\n",
    "    label_svhn= []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "\n",
    "            data2  =joint_data[1]\n",
    "            data2 = data2.float()\n",
    "\n",
    "            label1  =joint_data[2]\n",
    "            label2  =joint_data[3]\n",
    "            \n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data1 = data1.view(data1.size(0), -1)\n",
    "            data2 = data2.view(data2.size(0), -1)\n",
    "            \n",
    "            #z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD = model(data1,data2)  \n",
    "            z_posterior,reconstruction,mu,var,loss,  KLD       = model(data1,data2)\n",
    "            running_loss += loss.item()\n",
    "            running_mse1 += MSE1.item()\n",
    "            running_mse2 += MSE2.item()\n",
    "            running_kld  += KLD.item()    \n",
    "            \n",
    "            latent_repMNIST.append(z1_posterior)\n",
    "            latent_repSVHN.append(z2_posterior)\n",
    "            label_mnist.append(label1)\n",
    "            label_svhn.append(label2)\n",
    "\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data1.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction1.view(batch_size, 1, 28, 28)[:8]))\n",
    "                bothp = torch.cat((data2.view(batch_size, 3, 32, 32)[:8], \n",
    "                                  reconstruction2.view(batch_size, 3, 32, 32)[:8]))\n",
    "                save_image(both.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputMNIST_{epoch}.png\"), nrow=num_rows)\n",
    "                save_image(bothp.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputSVHN_{epoch}.png\"), nrow=num_rows)\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "    writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    latent_repMNIST = torch.vstack(latent_repMNIST).cpu().numpy()\n",
    "    latent_repSVHN  = torch.vstack(latent_repSVHN).cpu().numpy()\n",
    "    label_mnist     = torch.hstack(label_mnist).cpu().numpy()\n",
    "    label_svhn      = torch.hstack(label_svhn).cpu().numpy()\n",
    "    return test_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-aa326a2bdc71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} of {epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoint_dataset_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_repMNIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_repSVHN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_mnist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_svhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjoint_dataset_test_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-2b0ce673fb0a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, joint_dataloader, epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#        z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD       = model(data1,data2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mz_posterior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mKLD\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mrunning_mse1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mMSE1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrunning_mse2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mMSE2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achint-env2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 5\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,joint_dataset_train_loader,epoch)\n",
    "    test_epoch_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn = test(model,joint_dataset_test_loader,epoch)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
