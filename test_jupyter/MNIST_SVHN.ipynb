{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import gibbs_sampler_poise\n",
    "import kl_divergence_calculator\n",
    "import data_preprocessing\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import umap\n",
    "import random\n",
    "import shutil\n",
    "from numpy import prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# learning parameters\n",
    "latent_dim1 = 32\n",
    "latent_dim2 = 16\n",
    "batch_size = 10\n",
    "dim_MNIST   = 784\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tx = transforms.ToTensor()\n",
    "MNIST_TRAINING_PATH = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/training.pt\"\n",
    "SVHN_TRAINING_PATH  = \"/home/achint/Practice_code/VAE/SVHN/train_32x32.mat\"\n",
    "MNIST_TEST_PATH     = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/test.pt\"\n",
    "SVHN_TEST_PATH  = \"/home/achint/Practice_code/VAE/SVHN/test_32x32.mat\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "RECONSTRUCTION_PATH = \"/home/achint/Practice_code/Updated_POISE_VAE/MNIST_SVHN/reconstructions/\"\n",
    "PATH = \"/home/achint/Practice_code/Updated_POISE_VAE/MNIST_SVHN/mnist_svhn_parameters.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs directory and the reconstruction directory \n",
    "if os.path.exists(RECONSTRUCTION_PATH):\n",
    "    shutil.rmtree(RECONSTRUCTION_PATH)\n",
    "    os.makedirs(RECONSTRUCTION_PATH)\n",
    "\n",
    "if os.path.exists(SUMMARY_WRITER_PATH):\n",
    "    shutil.rmtree(SUMMARY_WRITER_PATH)\n",
    "    os.makedirs(SUMMARY_WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing MNIST and SVHN datasets\n",
    "joint_dataset_train=data_preprocessing.JointDataset(mnist_pt_path=MNIST_TRAINING_PATH,\n",
    "                             svhn_mat_path=SVHN_TRAINING_PATH)\n",
    "joint_dataset_test = data_preprocessing.JointDataset(mnist_pt_path=MNIST_TEST_PATH,\n",
    "                             svhn_mat_path=SVHN_TEST_PATH)\n",
    "\n",
    "joint_dataset_train_loader = DataLoader(\n",
    "    joint_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _latent_dims_type_setter(lds):\n",
    "    ret, ret_flatten = [], []\n",
    "    for ld in lds:\n",
    "        if hasattr(ld, '__iter__'): # Iterable\n",
    "            ld_tuple = tuple([i for i in ld])\n",
    "            if not all(map(lambda i: isinstance(i, int), ld_tuple)):\n",
    "                raise ValueError('`latent_dim` must be either iterable of ints or int.')\n",
    "            ret.append(ld_tuple)\n",
    "            ret_flatten.append(int(prod(ld_tuple)))\n",
    "        elif isinstance(ld, int):\n",
    "            ret.append((ld, ))\n",
    "            ret_flatten.append(ld)\n",
    "        else:\n",
    "            raise ValueError('`latent_dim` must be either iterable of ints or int.')\n",
    "    return ret, ret_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POISEVAE(nn.Module):\n",
    "    __version__ = 2.0\n",
    "    \n",
    "    def __init__(self, encoders, decoders, batch_size, loss, latent_dims=None,\n",
    "                 device=_device):\n",
    "        \"\"\"\n",
    "        encoders: list of nn.Module\n",
    "            Each encoder must have an attribute `latent_dim` specifying the dimension of the\n",
    "            latent space to which it encodes. An alternative way to avoid adding this attribute\n",
    "            is to specify the `latent_dims` parameter (see below). \n",
    "            Note that each `latent_dim` must be unsqueezed, e.g. (10, ) is not the same as (10, 1).\n",
    "            \n",
    "        decoders: list of nn.Module\n",
    "            The number and indices of decoders must match those of encoders.\n",
    "            \n",
    "        batch_size: int\n",
    "        \n",
    "        loss: str\n",
    "            Can either be 'MSE' for MSE loss or 'BCE' for BCE loss. The users should properly \n",
    "            restrict the range of the output of their decoders for the loss chosen.\n",
    "        \n",
    "        latent_dims: iterable, optional; default None\n",
    "            The dimensions of the latent spaces to which the encoders encode. The indices of the \n",
    "            entries must match those of encoders. An alternative way to specify the dimensions is\n",
    "            to add the attribute `latent_dim` to each encoder (see above).\n",
    "            Note that each entry must be unsqueezed, e.g. (10, ) is not the same as (10, 1).\n",
    "        \n",
    "        device: torch.device, optional\n",
    "        \"\"\"\n",
    "        super(POISEVAE,self).__init__()\n",
    "\n",
    "        if len(encoders) != len(decoders):\n",
    "            raise ValueError('The number of encoders must match that of decoders.')\n",
    "        \n",
    "        if len(encoders) > 2:\n",
    "            raise NotImplementedError('> 3 latent spaces not yet supported.')\n",
    "        \n",
    "        # Type check\n",
    "        if not all(map(lambda x: isinstance(x, nn.Module), (*encoders, *decoders))):\n",
    "            raise TypeError('`encoders` and `decoders` must be lists of `nn.Module` class.')\n",
    "\n",
    "        # Get the latent dimensions\n",
    "        if latent_dims is not None:\n",
    "            if not hasattr(latent_dims, '__iter__'): # Iterable\n",
    "                raise TypeError('`latent_dims` must be iterable.')\n",
    "            self.latent_dims = latent_dims\n",
    "        else:\n",
    "            self.latent_dims = tuple(map(lambda l: l.latent_dim, encoders))\n",
    "        self.latent_dims, self.latent_dims_flatten = _latent_dims_type_setter(self.latent_dims)\n",
    "\n",
    "        if batch_size <= 0:\n",
    "            raise ValueError('Invalid batch size')\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if loss not in ['MSE', 'BCE']: \n",
    "            raise NotImplementedError('Not yet supported for other loss functions')\n",
    "        self.loss = loss\n",
    "        \n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.decoders = nn.ModuleList(decoders)\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.gibbs = gibbs_sampler(self.latent_dims_flatten, batch_size)\n",
    "        self.kl_div = kl_divergence(self.latent_dims_flatten, batch_size)\n",
    "\n",
    "        self.register_parameter(name='g11', \n",
    "                                param=nn.Parameter(torch.randn(*self.latent_dims_flatten, \n",
    "                                                               device=self.device)))\n",
    "        self.register_parameter(name='g22', \n",
    "                                param=nn.Parameter(torch.randn(*self.latent_dims_flatten, \n",
    "                                                               device=self.device)))\n",
    "        self.flag_initialize = 1\n",
    "\n",
    "    def _decoder_helper(self):\n",
    "        \"\"\"\n",
    "        Reshape samples drawn from each latent space, and decode with considering the loss function\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        for decoder, z, ld in zip(self.decoders, self.z_gibbs_posteriors, self.latent_dims):\n",
    "            z = z.view(self.batch_size, *ld) # Match the shape to the output\n",
    "            x_ = decoder(z)\n",
    "            ret.append(x_)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Return\n",
    "        ------\n",
    "        results: dict\n",
    "            z: list of torch.Tensor\n",
    "                Samples from the posterior distributions in the corresponding latent spaces\n",
    "            x_rec: list of torch.Tensor\n",
    "                Reconstructed samples\n",
    "            mu: list of torch.Tensor\n",
    "                Posterior distribution means\n",
    "            var: list of torch.Tensor\n",
    "                Posterior distribution variances\n",
    "            total_loss: torch.Tensor\n",
    "            rec_losses: list of torch.tensor\n",
    "                Reconstruction loss for each dataset\n",
    "            KL_loss: torch.Tensor\n",
    "        \"\"\"\n",
    "        mu, var = [], []\n",
    "        for i, xi in enumerate(x):\n",
    "            _mu, _log_var = self.encoders[i].forward(xi)\n",
    "            mu.append(_mu.view(self.batch_size, -1))\n",
    "            var.append(-torch.exp(_log_var.view(self.batch_size, -1)))\n",
    "\n",
    "        g22 = -torch.exp(self.g22)\n",
    "\n",
    "        # Initializing gibbs sample\n",
    "        if self.flag_initialize == 1:\n",
    "            z_priors = self.gibbs.sample(self.g11, g22, n_iterations=5000)\n",
    "            z_posteriors = self.gibbs.sample(self.g11, g22, lambda1s=mu, lambda2s=var,\n",
    "                                             n_iterations=5000)\n",
    "\n",
    "            self.z_priors = z_priors\n",
    "            self.z_posteriors = z_posteriors\n",
    "            self.flag_initialize = 0\n",
    "\n",
    "        z_priors = list(map(lambda z: z.detach(), self.z_priors))\n",
    "        z_posteriors = list(map(lambda z: z.detach(), self.z_posteriors))\n",
    "\n",
    "        # If lambda not provided, treat as zeros to save memory and computation\n",
    "        self.z_gibbs_priors = self.gibbs.sample(self.g11, g22, z=z_priors, n_iterations=5)\n",
    "        self.z_gibbs_posteriors = self.gibbs.sample(self.g11, g22, lambda1s=mu, lambda2s=var,\n",
    "                                                    z=z_posteriors, n_iterations=5)\n",
    "\n",
    "        self.z_priors = list(map(lambda z: z.detach(), self.z_gibbs_priors))\n",
    "        self.z_posteriors = list(map(lambda z: z.detach(), self.z_gibbs_posteriors))\n",
    "\n",
    "        G = torch.block_diag(self.g11, self.g22)\n",
    "\n",
    "        x_ = self._decoder_helper() # Decoding\n",
    "\n",
    "        # self.z2_gibbs_posterior = self.z2_gibbs_posterior.squeeze()\n",
    "        for i in range(len(self.z_gibbs_posteriors)):\n",
    "            self.z_gibbs_posteriors[i] = self.z_gibbs_posteriors[i].squeeze()\n",
    "\n",
    "        # KL loss\n",
    "        kls = self.kl_div.calc(G, self.z_gibbs_posteriors, self.z_gibbs_priors, mu,var)\n",
    "        KL_loss  = sum(kls)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        rec_loss_func = nn.MSELoss(reduction='sum') if self.loss == 'MSE' else \\\n",
    "                        nn.BCELoss(reduction='sum')\n",
    "        recs = list(map(lambda x: rec_loss_func(x[0], x[1]), zip(x_, x)))\n",
    "        rec_loss = sum(recs)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = KL_loss + rec_loss\n",
    "\n",
    "        results = {\n",
    "            'z': self.z_posteriors, 'x_rec': x_, 'mu': mu, 'var': var, \n",
    "            'total_loss': total_loss, 'rec_losses': recs, 'KL_loss': KL_loss\n",
    "        }\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.latent_dim = 32\n",
    "        self.dim_MNIST   = 784\n",
    "\n",
    "        ## Encoder set1(MNIST)\n",
    "        self.set1_enc1 = nn.Linear(in_features = self.dim_MNIST,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*self.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Modality 1 (MNIST)\n",
    "        x       = F.relu(self.set1_enc1(x))\n",
    "        x       = F.relu(self.set1_enc2(x))  \n",
    "        x       = self.set1_enc3(x).view(-1,2,self.latent_dim)  # ->[128,2,32]\n",
    "        mu      = x[:,0,:] # ->[128,32]\n",
    "        log_var = x[:,1,:] # ->[128,32]\n",
    "        var     = -torch.exp(log_var)           #lambdap_2<0\n",
    "        return mu, log_var\n",
    "    \n",
    "class Encoder2(nn.Module):\n",
    "    # 64*64 -> 40*40 -> 16*16 -> 4*4\n",
    "    def __init__(self):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.latent_dim = 16     \n",
    "        # input size: 3 x 32 x 32\n",
    "        self.set2_enc1 = nn.Conv2d(in_channels=3, out_channels=2*self.latent_dim, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_enc2 = nn.Conv2d(in_channels=2*self.latent_dim, out_channels=2*self.latent_dim, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_enc3 = nn.Conv2d(in_channels=2*self.latent_dim, out_channels=self.latent_dim, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 16 x 4 x 4   \n",
    "        self.SVHNc1 = nn.Conv2d(self.latent_dim, self.latent_dim, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.SVHNc2 = nn.Conv2d(self.latent_dim, self.latent_dim, 4, 1, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "    def forward(self, x):\n",
    "        # Modality 2 (SVHN)\n",
    "        x = x.view(-1,3, 32,32) \n",
    "        x = F.relu(self.set2_enc1(x))\n",
    "        x = F.relu(self.set2_enc2(x))\n",
    "        x = F.relu(self.set2_enc3(x))\n",
    "        # get 'mu' and 'log_var' for SVHN\n",
    "        mu = (self.SVHNc1(x).squeeze(3)).squeeze(2)\n",
    "        log_var = (self.SVHNc2(x).squeeze(3)).squeeze(2)\n",
    "        return mu, log_var\n",
    "    \n",
    "class Decoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder1, self).__init__()  \n",
    "        self.latent_dim = 32\n",
    "        self.dim_MNIST   = 784\n",
    "        ## Decoder set1(MNIST)\n",
    "        self.set1_dec1 = nn.Linear(in_features = self.latent_dim,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = self.dim_MNIST)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.set1_dec1(x))\n",
    "        x = self.set1_dec2(x) \n",
    "        x = self.set1_dec3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class Decoder2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder2, self).__init__()  \n",
    "        self.latent_dim = 16    \n",
    "        ## Decoder set2(SVHN)\n",
    "        # input size: 16x1x1\n",
    "        self.set2_dec0 = nn.ConvTranspose2d(in_channels=self.latent_dim,out_channels=self.latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "        # input size: 16x4x4\n",
    "        self.set2_dec1 = nn.ConvTranspose2d(in_channels=self.latent_dim,out_channels=2*self.latent_dim, kernel_size=3, stride=1, padding=1)\n",
    "        # size: 32 x 4 x 4\n",
    "        self.set2_dec2 = nn.ConvTranspose2d(in_channels=2*self.latent_dim,out_channels=2*self.latent_dim, kernel_size=5, stride=1, padding=0)\n",
    "        # size: 32 x 8 x 8\n",
    "        self.set2_dec3 = nn.ConvTranspose2d(in_channels=2*self.latent_dim,out_channels=2*self.latent_dim, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 32 x 16 x 16\n",
    "        self.set2_dec4 = nn.ConvTranspose2d(in_channels=2*self.latent_dim,out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 3 x 32 x 32\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.set2_dec0(x))\n",
    "        x = F.relu(self.set2_dec1(x))\n",
    "        x = F.relu(self.set2_dec2(x))\n",
    "        x = F.relu(self.set2_dec3(x))\n",
    "        x = self.set2_dec4(x).view(-1,3072)\n",
    "        return x\n",
    "    \n",
    "enc1 = Encoder1()\n",
    "enc2 = Encoder2().to(_device)\n",
    "dec1 = Decoder1().to(_device)\n",
    "dec2 = Decoder2().to(_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g11\n",
      "g22\n",
      "encoders.0.set1_enc1.weight\n",
      "encoders.0.set1_enc1.bias\n",
      "encoders.0.set1_enc2.weight\n",
      "encoders.0.set1_enc2.bias\n",
      "encoders.0.set1_enc3.weight\n",
      "encoders.0.set1_enc3.bias\n",
      "encoders.1.set2_enc1.weight\n",
      "encoders.1.set2_enc1.bias\n",
      "encoders.1.set2_enc2.weight\n",
      "encoders.1.set2_enc2.bias\n",
      "encoders.1.set2_enc3.weight\n",
      "encoders.1.set2_enc3.bias\n",
      "encoders.1.SVHNc1.weight\n",
      "encoders.1.SVHNc1.bias\n",
      "encoders.1.SVHNc2.weight\n",
      "encoders.1.SVHNc2.bias\n",
      "decoders.0.set1_dec1.weight\n",
      "decoders.0.set1_dec1.bias\n",
      "decoders.0.set1_dec2.weight\n",
      "decoders.0.set1_dec2.bias\n",
      "decoders.0.set1_dec3.weight\n",
      "decoders.0.set1_dec3.bias\n",
      "decoders.1.set2_dec0.weight\n",
      "decoders.1.set2_dec0.bias\n",
      "decoders.1.set2_dec1.weight\n",
      "decoders.1.set2_dec1.bias\n",
      "decoders.1.set2_dec2.weight\n",
      "decoders.1.set2_dec2.bias\n",
      "decoders.1.set2_dec3.weight\n",
      "decoders.1.set2_dec3.bias\n",
      "decoders.1.set2_dec4.weight\n",
      "decoders.1.set2_dec4.bias\n"
     ]
    }
   ],
   "source": [
    "state = torch.load(PATH)\n",
    "model = POISEVAE([enc1, enc2], [dec1, dec2], batch_size, loss='MSE', latent_dims=[32, (16, 1, 1)]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "model.load_state_dict(state['state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "for name, para in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,joint_dataloader,epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i,joint_data in enumerate(joint_dataloader):\n",
    "        data1    = joint_data[0]\n",
    "        data1    = data1.float()\n",
    "        data2   = joint_data[1]\n",
    "        data2   = data2.float()\n",
    "        data1    = data1.to(device)\n",
    "        data2   = data2.to(device)\n",
    "        data1    = data1.view(data1.size(0), -1)\n",
    "        data2   = data2.view(data2.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        results = model([data1,data2])\n",
    "        z_posterior, mu, var = results['z'], results['mu'], results['var']\n",
    "        reconstruction = results['x_rec']\n",
    "        total_loss, rec_loss, KLD = results['total_loss'], results['rec_losses'], results['KL_loss']\n",
    "\n",
    "        running_mse1 += rec_loss[0].item()\n",
    "        running_mse2 += rec_loss[1].item()\n",
    "        running_kld  += KLD.item()\n",
    "        running_loss += total_loss.item()          #.item converts tensor with one element to number\n",
    "        total_loss.backward()                      #.backward\n",
    "        optimizer.step()                     #.step one learning step\n",
    "    train_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "#     writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "#     writer.add_scalar(\"training/MSE1\", mse1_loss, epoch)\n",
    "#     writer.add_scalar(\"training/MSE2\", mse2_loss, epoch)\n",
    "#     writer.add_scalar(\"training/KLD\", kld_loss, epoch)    \n",
    "    return train_loss\n",
    "    \n",
    "def test(model,joint_dataloader,epoch):\n",
    "    latent_repMNIST= []\n",
    "    latent_repSVHN= []\n",
    "    label_mnist= []\n",
    "    label_svhn= []\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "\n",
    "            data2  =joint_data[1]\n",
    "            data2 = data2.float()\n",
    "\n",
    "            label1  =joint_data[2]\n",
    "            label2  =joint_data[3]\n",
    "            \n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            data1 = data1.view(data1.size(0), -1)\n",
    "            data2 = data2.view(data2.size(0), -1)            \n",
    "            z_posterior,reconstruction,mu,var,total_loss, rec_loss,  KLD       = model([data1,data2])\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "            running_mse1 += rec_loss[0].item()\n",
    "            running_mse2 += rec_loss[0].item()\n",
    "            running_kld  += KLD.item()    \n",
    "            \n",
    "            latent_repMNIST.append(z_posterior[0])\n",
    "            latent_repSVHN.append(z_posterior[1])\n",
    "            label_mnist.append(label1)\n",
    "            label_svhn.append(label2)\n",
    "\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data1.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction[0].view(batch_size, 1, 28, 28)[:8]))\n",
    "                bothp = torch.cat((data2.view(batch_size, 3, 32, 32)[:8], \n",
    "                                  reconstruction[1].view(batch_size, 3, 32, 32)[:8]))\n",
    "                save_image(both.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputMNIST_{epoch}.png\"), nrow=num_rows)\n",
    "                save_image(bothp.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"1_outputSVHN_{epoch}.png\"), nrow=num_rows)\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "#     writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "#     writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "#     writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    latent_repMNIST = torch.vstack(latent_repMNIST).cpu().numpy()\n",
    "    latent_repSVHN  = torch.vstack(latent_repSVHN).cpu().numpy()\n",
    "    label_mnist     = torch.hstack(label_mnist).cpu().numpy()\n",
    "    label_svhn      = torch.hstack(label_svhn).cpu().numpy()\n",
    "    return test_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 5\n",
      "Train Loss: 63.0024\n",
      "Test Loss: 62.5679\n",
      "Epoch 2 of 5\n",
      "Train Loss: 60.6707\n",
      "Test Loss: 60.3820\n",
      "Epoch 3 of 5\n",
      "Train Loss: 58.1589\n",
      "Test Loss: 58.7737\n",
      "Epoch 4 of 5\n",
      "Train Loss: 56.7420\n",
      "Test Loss: 57.0718\n",
      "Epoch 5 of 5\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 5\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,joint_dataset_train_loader,epoch)\n",
    "    test_epoch_loss,latent_repMNIST,latent_repSVHN,label_mnist,label_svhn = test(model,joint_dataset_test_loader,epoch)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()}\n",
    "torch.save(state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
