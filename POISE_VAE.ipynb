{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bcdc52a-4497-4883-ac34-b03e17b25ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import gibbs_sampler_poise\n",
    "# import kl_divergence_calculator\n",
    "from numpy import prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31346061-38ef-4249-a36a-11bea271e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def _latent_dims_type_setter(lds):\n",
    "    ret, ret_flatten = [], []\n",
    "    for ld in lds:\n",
    "        if hasattr(ld, '__iter__'): # Iterable\n",
    "            ld_tuple = tuple([i for i in ld])\n",
    "            if not all(map(lambda i: isinstance(i, int), ld_tuple)):\n",
    "                raise ValueError('`latent_dim` must be either iterable of ints or int.')\n",
    "            ret.append(ld_tuple)\n",
    "            ret_flatten.append(int(prod(ld_tuple)))\n",
    "        elif isinstance(ld, int):\n",
    "            ret.append((ld, ))\n",
    "            ret_flatten.append(ld)\n",
    "        else:\n",
    "            raise ValueError('`latent_dim` must be either iterable of ints or int.')\n",
    "    return ret, ret_flatten\n",
    "\n",
    "\n",
    "class POISEVAE(nn.Module):\n",
    "    def __init__(self, encoders, decoders, batch_size, latent_dims=None, use_mse_loss=True,\n",
    "                 device=_device):\n",
    "        \"\"\"\n",
    "        encoders: list of nn.Module\n",
    "            Each encoder must have an attribute `latent_dim` specifying the dimension of the\n",
    "            latent space to which it encodes. An alternative way to avoid adding this attribute\n",
    "            is to specify the `latent_dims` parameter (see below). \n",
    "            Note that each `latent_dim` must be unsqueezed, e.g. (10, ) is not the same as (10, 1).\n",
    "            \n",
    "        decoders: list of nn.Module\n",
    "            The number and indices of decoders must match those of encoders.\n",
    "            \n",
    "        batch_size: int\n",
    "        \n",
    "        latent_dims: iterable, optional; default None\n",
    "            The dimensions of the latent spaces to which the encoders encode. The indices of the \n",
    "            entries must match those of encoders. An alternative way to specify the dimensions is\n",
    "            to add the attribute `latent_dim` to each encoder (see above).\n",
    "            Note that each entry must be unsqueezed, e.g. (10, ) is not the same as (10, 1).\n",
    "        \n",
    "        use_mse_loss: boolean, optional; default True\n",
    "            To use MSE loss or not; if not, BCE loss will be used.\n",
    "        \n",
    "        device: torch.device, optional\n",
    "        \"\"\"\n",
    "        super(POISEVAE,self).__init__()\n",
    "\n",
    "        if len(encoders) != len(decoders):\n",
    "            raise ValueError('The number of encoders must match that of decoders.')\n",
    "        \n",
    "        if len(encoders) > 2:\n",
    "            raise NotImplementedError('> 3 latent spaces not yet supported.')\n",
    "        \n",
    "        # Type check\n",
    "        if not all(map(lambda x: isinstance(x, nn.Module), (*encoders, *decoders))):\n",
    "            raise TypeError('`encoders` and `decoders` must be lists of `nn.Module` class.')\n",
    "\n",
    "        # Get the latent dimensions\n",
    "        if latent_dims is not None:\n",
    "            if not hasattr(latent_dims, '__iter__'): # Iterable\n",
    "                raise TypeError('`latent_dims` must be iterable.')\n",
    "            self.latent_dims = latent_dims\n",
    "        else:\n",
    "            self.latent_dims = tuple(map(lambda l: l.latent_dim, encoders))\n",
    "        self.latent_dims, self.latent_dims_flatten = _latent_dims_type_setter(self.latent_dims)\n",
    "\n",
    "        self.encoders = encoders\n",
    "        self.decoders = decoders\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.gibbs = gibbs_sampler(self.latent_dims_flatten, batch_size)\n",
    "        self.kl_div = kl_divergence(self.latent_dims_flatten, batch_size)\n",
    "\n",
    "        self.register_parameter(name='g11', \n",
    "                                param=nn.Parameter(torch.randn(*self.latent_dims_flatten, \n",
    "                                                               device=self.device)))\n",
    "        self.register_parameter(name='g22', \n",
    "                                param=nn.Parameter(torch.randn(*self.latent_dims_flatten, \n",
    "                                                               device=self.device)))\n",
    "        self.flag_initialize = 1\n",
    "\n",
    "    def _decoder_helper(self):\n",
    "        \"\"\"\n",
    "        Reshape samples drawn from each latent space, and decode with considering the loss function\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        for decoder, z, ld in zip(self.decoders, self.z_gibbs_posteriors, self.latent_dims):\n",
    "            z = z.view(self.batch_size, *ld) # Match the shape to the output\n",
    "            x_ = decoder(z)\n",
    "            if not self.use_mse_loss: # BCE instead\n",
    "                x_ = torch.sigmoid(x_)\n",
    "            ret.append(x_)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, var = [], []\n",
    "        for i, xi in enumerate(x):\n",
    "            _mu, _log_var = self.encoders[i].forward(xi)\n",
    "            mu.append(_mu.view(self.batch_size, -1))\n",
    "            var.append(-torch.exp(_log_var.view(self.batch_size, -1)))\n",
    "\n",
    "        g22 = -torch.exp(self.g22)\n",
    "\n",
    "        # Initializing gibbs sample\n",
    "        if self.flag_initialize == 1:\n",
    "            z_priors = self.gibbs.sample(self.g11, g22, n_iterations=5000)\n",
    "            z_posteriors = self.gibbs.sample(self.g11, g22, lambda1s=mu, lambda2s=var,\n",
    "                                             n_iterations=5000)\n",
    "\n",
    "            self.z_priors = z_priors\n",
    "            self.z_posteriors = z_posteriors\n",
    "            self.flag_initialize = 0\n",
    "\n",
    "        z_priors = list(map(lambda z: z.detach(), self.z_priors))\n",
    "        z_posteriors = list(map(lambda z: z.detach(), self.z_posteriors))\n",
    "\n",
    "        # If lambda not provided, treat as zeros to save memory and computation\n",
    "        self.z_gibbs_priors = self.gibbs.sample(self.g11, g22, z=z_priors, n_iterations=5)\n",
    "        self.z_gibbs_posteriors = self.gibbs.sample(self.g11, g22, lambda1s=mu, lambda2s=var,\n",
    "                                                    z=z_posteriors, n_iterations=5)\n",
    "\n",
    "        self.z_priors = list(map(lambda z: z.detach(), self.z_gibbs_priors))\n",
    "        self.z_posteriors = list(map(lambda z: z.detach(), self.z_gibbs_posteriors))\n",
    "\n",
    "        G = torch.block_diag(self.g11, self.g22)\n",
    "\n",
    "        x_ = self._decoder_helper() # Decoding\n",
    "\n",
    "        # self.z2_gibbs_posterior = self.z2_gibbs_posterior.squeeze()\n",
    "        for i in range(len(self.z_gibbs_posteriors)):\n",
    "            self.z_gibbs_posteriors[i] = self.z_gibbs_posteriors[i].squeeze()\n",
    "\n",
    "        # KL loss\n",
    "        kls = self.kl_div.calc(G, self.z_gibbs_posteriors, self.z_gibbs_priors, mu,var)\n",
    "        KL_loss  = sum(kls)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        loss_func = nn.MSELoss(reduction='sum') if self.use_mse_loss else nn.BCELoss(reduction='sum')\n",
    "        recs = list(map(lambda x: loss_func(x[0], x[1]), zip(x_, x)))\n",
    "        rec_loss = sum(recs)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = KL_loss + rec_loss\n",
    "\n",
    "        return self.z_posteriors, x_, mu, var, loss, recs, KL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc9c912-2d0c-4690-83b2-950ca5dd66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kl_divergence():\n",
    "    def __init__(self, latent_dims, batch_size, device=_device):\n",
    "        self.latent_dims = latent_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def calc(self, G, z, z_priors, mu, var):\n",
    "        ## Creating Sufficient statistics\n",
    "        T_priors, T_posts, lambdas = [], [], []\n",
    "        for z_i, z_prior_i, mu_i, var_i in zip(z, z_priors, mu, var):\n",
    "            T_priors.append(torch.cat((z_prior_i, torch.square(z_prior_i)), 1))\n",
    "            T_posts.append(torch.cat((z_i, torch.square(z_i)), 1))\n",
    "            lambdas.append(torch.cat((mu_i,var_i),1))\n",
    "            \n",
    "        # TODO: make it generic for > 2 latent spaces\n",
    "        T_prior_sqrd = torch.sum(torch.square(z_priors[0]), 1) + \\\n",
    "                       torch.sum(torch.square(z_priors[1]), 1) #stores z^2+z'^2\n",
    "        T_post_sqrd  = torch.sum(torch.square(z[0]), 1) + \\\n",
    "                       torch.sum(torch.square(z[1]), 1)\n",
    "        T1_prior_unsq = T_priors[0].unsqueeze(2)       \n",
    "        T2_prior_unsq = T_priors[1].unsqueeze(1)       \n",
    "        T1_post_unsq  = T_posts[0].unsqueeze(2)        \n",
    "        T2_post_unsq  = T_posts[1].unsqueeze(1)        \n",
    "        T_prior_kron = torch.zeros(self.batch_size, 2 * self.latent_dims[0], \n",
    "                                   2 * self.latent_dims[1]).to(self.device)\n",
    "        T_post_kron = torch.zeros(T_prior_kron.shape).to(self.device)\n",
    "       \n",
    "        for i in range(self.batch_size):\n",
    "            T_prior_kron[i,:] = torch.kron(T1_prior_unsq[i,:], T2_prior_unsq[i,:])\n",
    "            T_post_kron[i,:] = torch.kron(T1_post_unsq[i,:], T2_post_unsq[i,:])    \n",
    "            \n",
    "        part_fun0 = self.dot_product(lambdas[0], T_posts[0]) + \\\n",
    "                    self.dot_product(lambdas[1], T_posts[1])\n",
    "        part_fun1 = -self.dot_product(lambdas[0], T_posts[0].detach()) - \\\n",
    "                     self.dot_product(lambdas[1], T_posts[1].detach()) #-lambda*Tq-lambda'Tq'    \n",
    "        part_fun2 = self.dot_product(T_prior_kron.detach(), G) - \\\n",
    "                    self.dot_product(T_post_kron.detach(), G)\n",
    "\n",
    "        return part_fun0, part_fun1, part_fun2\n",
    "    \n",
    "    def dot_product(self, tensor_1, tensor_2):\n",
    "        out = torch.sum(torch.mul(tensor_1, tensor_2))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbad29d5-af95-455d-ba63-2ac3bec18ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gibbs_sampler():\n",
    "    def __init__(self, latent_dims, batch_size, device=_device):\n",
    "        self.latent_dims = latent_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def var_calc(self,z, g22, lambda_2):\n",
    "        val = 1 - torch.matmul(torch.square(z), g22)\n",
    "        if lambda_2 is not None:\n",
    "            val -= lambda_2\n",
    "        return torch.reciprocal(2 * val)\n",
    "\n",
    "    def mean_calc(self, z, var, g11, lambda_1):\n",
    "        beta = torch.matmul(z, g11)\n",
    "        if lambda_1 is not None:\n",
    "            beta += lambda_1\n",
    "        return var * beta\n",
    "\n",
    "    def value_calc(self,z, g11, g22, lambda_1, lambda_2):\n",
    "        var1 = self.var_calc(z, g22, lambda_2)\n",
    "        mean1 = self.mean_calc(z, var1, g11, lambda_1)\n",
    "        out = mean1 + torch.sqrt(var1.float()) * torch.randn_like(var1)\n",
    "        return out\n",
    "\n",
    "    def sample(self, g11, g22, z=None, lambda1s=None, lambda2s=None, n_iterations=1):\n",
    "        \"\"\"\n",
    "        g11, g22: \n",
    "            Diagonal blocks of the metric tensor\n",
    "        z: \n",
    "            If not provided, randomly initialize\n",
    "        lambda1s: optional\n",
    "            Natural parameter 1 of the latent distributions\n",
    "            If not provided, treat as zeros\n",
    "        lambda1s: optional\n",
    "            Natural parameter 2 of the latent distributions\n",
    "            If not provided, treat as zeros\n",
    "        n_iterations: int, optional; default 1\n",
    "        \"\"\"\n",
    "            # TODO: function signature of gibbs_sample: optional parameters\n",
    "            # flag_init. not necessary; if z not provided, init. z rand.ly\n",
    "            # Not really an optimization but make the code clear\n",
    "            # in case people want to look carefully in the future\n",
    "            # I made an attempt in the local file `gibbs_sampler_poise.py`; debugging needed\n",
    "        if z is None:\n",
    "            z = [torch.randn(self.batch_size, ld).squeeze().to(self.device) \n",
    "                 for ld in self.latent_dims]\n",
    "        if lambda1s is None:\n",
    "            lambda1s = [None for _ in range(len(self.latent_dims))]\n",
    "        if lambda2s is None:\n",
    "            lambda2s = [None for _ in range(len(self.latent_dims))]\n",
    "\n",
    "        # TODO: make it generic for > 2 latent spaces \n",
    "        for i in range(n_iterations):\n",
    "            z[0] = self.value_calc(z[1], torch.transpose(g11,0,1), torch.transpose(g22,0,1),\n",
    "                                   lambda1s[0], lambda2s[0]) \n",
    "            z[1] = self.value_calc(z[0], g11, g22, lambda1s[1], lambda2s[1])\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5d16c8-0150-41d0-99c7-e5a4f9651389",
   "metadata": {},
   "outputs": [],
   "source": [
    "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76d9082-640d-4f46-8256-418e60292b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class Encoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.l1 = nn.Linear(100, 50).to(_device)\n",
    "        self.l2mu = nn.Linear(50, 10).to(_device)\n",
    "        self.l2var = nn.Linear(50, 10).to(_device)\n",
    "        self.latent_dim = 10\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        mu = self.l2mu(x)\n",
    "        log_var = self.l2var(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "class Encoder2(nn.Module):\n",
    "    # 64*64 -> 40*40 -> 16*16 -> 4*4\n",
    "    def __init__(self):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.l1 = nn.Conv2d(3, 2, (25, 25)).to(_device)\n",
    "        self.l2 = nn.Conv2d(2, 1, (25, 25)).to(_device)\n",
    "        self.l2mu = nn.Conv2d(1, 1, (13, 13)).to(_device)\n",
    "        self.l2var = nn.Conv2d(1, 1, (13, 13)).to(_device)\n",
    "        self.latent_dim = (1, 4, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        mu = self.l2mu(x)\n",
    "        log_var = self.l2var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "enc1 = Encoder1()\n",
    "dec1 = nn.Sequential(nn.Linear(10, 50), nn.Linear(50, 100)).to(_device)\n",
    "\n",
    "\n",
    "enc2 = Encoder2()\n",
    "dec2 = nn.Sequential(nn.ConvTranspose2d(1, 1, (13, 13)), \n",
    "                     nn.ConvTranspose2d(1, 2, (25, 25)), \n",
    "                     nn.ConvTranspose2d(2, 3, (25, 25))).to(_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e1bf63-57a8-4992-956c-0558e2f95b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = POISE_VAE.POISEVAE([enc1, enc2], [dec1, dec2], batch_size=10, use_mse_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26c9452-db2f-4e5b-9db7-b67e800b64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = torch.randn(10, 100, device=_device)\n",
    "data2 = torch.randn(10, 3, 64, 64, device=_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1b2b99-6ce1-4683-ae86-2f3dbb68e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = net([data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469e692-64cc-464b-8592-1fe08ab017d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
