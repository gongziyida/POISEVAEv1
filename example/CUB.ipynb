{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b49d1a-2833-46a5-b8f3-c43ae1c123e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F  # activation function\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import poisevae\n",
    "from poisevae.datasets import CUB\n",
    "from poisevae.utils import NN_lookup\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "plt.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617bd66-2c15-404b-9b8c-31a9b3a2e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_PATH = os.path.expanduser('~')\n",
    "DATA_PATH = os.path.join(HOME_PATH, 'Datasets/CUB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45980a9b-ce22-4cd1-b8a7-634da8c811c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncImg(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=2048):\n",
    "        super(EncImg, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        n = (input_dim, 1024, 512, 256)\n",
    "        li = []\n",
    "        for i in range(len(n)-1):\n",
    "            li += [nn.Linear(n[i], n[i+1]), nn.ELU(inplace=True)]\n",
    "        self.enc = nn.Sequential(*li)\n",
    "        self.enc_mu = nn.Linear(256, latent_dim)\n",
    "        self.enc_var = nn.Linear(256, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        mu = self.enc_mu(x)\n",
    "        log_var = self.enc_var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "class DecImg(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim=2048):\n",
    "        super(DecImg, self).__init__()  \n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        n = (output_dim, 1024, 512, 256)\n",
    "        li = [nn.Linear(latent_dim, 256)]\n",
    "        for i in range(len(n)-1, 0, -1):\n",
    "            li += [nn.ELU(inplace=True), nn.Linear(n[i], n[i-1])]\n",
    "        self.dec = nn.Sequential(*li)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.dec(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27158ccf-6906-4321-ad56-34a932777d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncTxt(nn.Module):\n",
    "    def __init__(self, vocab_size, latent_dim, emb_dim=128):\n",
    "        super(EncTxt, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        # 0 is for the excluded words and does not contribute to gradient\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        n_channels = (1, 32, 64, 128, 256, 512)\n",
    "        kernels = (4, 4, 4, (1, 4), (1, 4))\n",
    "        strides = (2, 2, 2, (1, 2), (1, 2))\n",
    "        paddings = (1, 1, 1, (0, 1), (0, 1))\n",
    "        li = []\n",
    "        for i, (n, k, s, p) in enumerate(zip(n_channels[1:], kernels, strides, paddings), 1):\n",
    "            li += [nn.Conv2d(n_channels[i-1], n, kernel_size=k, stride=s, padding=p), \n",
    "                   nn.BatchNorm2d(n), nn.ReLU(inplace=True)]\n",
    "            \n",
    "        self.enc = nn.Sequential(*li)\n",
    "        self.enc_mu = nn.Conv2d(512, latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "        self.enc_var = nn.Conv2d(512, latent_dim, kernel_size=4, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x.long()).unsqueeze(1) # add channel dim\n",
    "        x = self.enc(x)\n",
    "        mu = self.enc_mu(x).squeeze()\n",
    "        log_var = self.enc_var(x).squeeze()\n",
    "        return mu, log_var\n",
    "\n",
    "class DecTxt(nn.Module):\n",
    "    def __init__(self, vocab_size, latent_dim, emb_dim=128, txt_len=32):\n",
    "        super(DecTxt, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.txt_len = txt_len\n",
    "        \n",
    "        n_channels = (1, 32, 64, 128, 256, 512, latent_dim)\n",
    "        kernels = (4, 4, 4, (1, 4), (1, 4), 4)\n",
    "        strides = (2, 2, 2, (1, 2), (1, 2), 1)\n",
    "        paddings = (1, 1, 1, (0, 1), (0, 1), 0)\n",
    "        li = []\n",
    "        for i, (n, k, s, p) in enumerate(zip(n_channels[1:], kernels, strides, paddings), 1):\n",
    "            li = [nn.ConvTranspose2d(n, n_channels[i-1], kernel_size=k, stride=s, padding=p), \n",
    "                  nn.BatchNorm2d(n_channels[i-1]), nn.ReLU(inplace=True)] + li\n",
    "            \n",
    "        # No batchnorm at the first and last block\n",
    "        del li[-2]\n",
    "        del li[1]\n",
    "        self.dec = nn.Sequential(*li)\n",
    "        self.anti_emb = nn.Linear(emb_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, z, argmax=False):\n",
    "        z = self.dec(z)\n",
    "        z = self.anti_emb(z.view(-1, self.emb_dim))\n",
    "        z = z.view(-1, self.txt_len, self.vocab_size) # batch x txt len x vocab size\n",
    "        if argmax:\n",
    "            z = z.argmax(-1).float()\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0bb9d-30b5-4e79-be48-e46ee2b8a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = lambda data: torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9b1ec-5083-4a19-affa-e0720b60d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0513a4f-e792-4606-a624-478562027aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUB_train = CUB(DATA_PATH, DATA_PATH, 'train', device, tx, return_idx=False)\n",
    "CUB_test = CUB(DATA_PATH, DATA_PATH, 'test', device, tx, return_idx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88947a74-5dfc-4181-a9ce-c0dbdd9ae419",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, txt_len = CUB_train.CUBtxt.vocab_size, CUB_train.CUBtxt.max_sequence_length\n",
    "vocab_size, txt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e601a8c-e46f-4cf0-9985-f2431788ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(CUB_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(CUB_test, batch_size=batch_size, shuffle=True)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68120dc-2f9e-49b1-8dfc-579032c915b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_img = EncImg(128).to(device)\n",
    "dec_img = DecImg(128).to(device)\n",
    "enc_txt = EncTxt(vocab_size, 128).to(device)\n",
    "dec_txt = DecTxt(vocab_size, 128).to(device)\n",
    "\n",
    "def CELoss(input, target):\n",
    "    batch_size = target.shape[0]\n",
    "    input, target = input.view(-1, vocab_size), target.view(-1).to(torch.long)\n",
    "    loss = nn.functional.cross_entropy(input, target, reduction='sum') #/ batch_size\n",
    "    return loss\n",
    "    \n",
    "def MSELoss(input, target):\n",
    "    loss = nn.functional.mse_loss(input, target, reduction='sum') #/ batch_size\n",
    "    return loss / 10\n",
    "    \n",
    "def MAELoss(input, target):\n",
    "    loss = nn.functional.l1_loss(input, target, reduction='sum') #/ batch_size\n",
    "    return loss / 10\n",
    "    \n",
    "vae = poisevae.POISEVAE([enc_img, enc_txt], [dec_img, dec_txt], [MAELoss, CELoss], \n",
    "                        latent_dims=[128, (128, 1, 1)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101efaf3-fe96-4c0c-96d0-2bc826f15cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in vae.named_parameters():\n",
    "#     print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292dc208-4258-4108-ae4d-80d954ab45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12663d4c-8f82-4b21-bed4-67044e7ea7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(HOME_PATH, 'Datasets/CUB_train_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f75ff-8b35-4e7a-b25d-324a43c1040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "# try:\n",
    "#     vae, optimizer, epoch = poisevae.utils.load_checkpoint(vae, optimizer, sorted(glob.glob(PATH + 'train*.pt'))[-1])\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381c092-1abf-46a5-8d6f-bee8ce5cb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "writer = SummaryWriter('runs/CUB/' + datetime.now().strftime('%y%m%d%H%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9667dd7-6be8-403c-aa1c-567307eea886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 30 + epoch\n",
    "for epoch in tqdm(range(epoch, epochs)):\n",
    "    poisevae.utils.train(vae, train_loader, optimizer, epoch, writer)\n",
    "    labels, latent_info = poisevae.utils.test(vae, test_loader, epoch, writer, \n",
    "                                              record_idx=(2, 3), return_latents=True)\n",
    "    if (epoch+1) % 10 == 0 and epoch > 0:\n",
    "        poisevae.utils.save_checkpoint(vae, optimizer, PATH + 'training_%d.pt' % (epoch+1), epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f67ff-6c71-44b1-947e-44037a6a8111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea1deb-19d1-49c9-b6ce-37b5594a2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisevae.utils.save_latent_info(latent_info, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71549c9c-3cc9-41ae-aeaf-f32da3946ffb",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6190b-67b5-4c7f-b1f9-a86fbd749573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae, optimizer, epoch = poisevae.utils.load_checkpoint(vae, optimizer, sorted(glob.glob(PATH + 'train*.pt'))[-1])\n",
    "# epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26711b10-747c-4df5-a4a5-9ba9ec35039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (img, txt, idx) in enumerate(test_loader):\n",
    "        results = vae([img.to(device), txt.to(device)])\n",
    "        dist, idx_h = NN_lookup(results['x_rec'][0], img)\n",
    "        # dist, idx_h = NN_lookup(results['x_rec'][0], img)\n",
    "        imgs_h = [CUB_test.CUBimg.dataset[int(idx[j]) // 10][0] for j in idx_h]\n",
    "        imgs = [CUB_test.CUBimg.dataset[int(j) // 10][0] for j in idx]\n",
    "        txts = [CUB_test.CUBtxt.data[str(int(j))] for j in idx]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b620c-e362-4afc-82e8-5ae452805d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = min(len(imgs_h), 20)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=ncols, figsize=(15, 1.5))\n",
    "for i, aux in enumerate(zip(imgs, imgs_h)):\n",
    "    if i >= ncols:\n",
    "        break\n",
    "    for j, im in enumerate(aux):\n",
    "        ax[j, i].imshow(im.permute(1, 2, 0))\n",
    "        ax[j, i].set_xticks([])\n",
    "        ax[j, i].set_yticks([])\n",
    "ax[1, 0].set_ylabel('Rec', fontsize=24)\n",
    "fig.tight_layout(pad=0)\n",
    "fig.savefig(PATH + 'CUBImgRec.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4bfe12-172d-49bc-a943-bac8910cecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = min(len(imgs_h), 20)\n",
    "nrows = 2 * int(np.floor(len(imgs_h) / 20))\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 1.5*nrows/2))\n",
    "for i, aux in enumerate(zip(imgs, imgs_h)):\n",
    "    if i >= nrows / 2 * 20:\n",
    "        break\n",
    "    for j, im in enumerate(aux):\n",
    "        ax[j+i//20*2, i%20].imshow(im.permute(1, 2, 0))\n",
    "        ax[j+i//20*2, i%20].set_xticks([])\n",
    "        ax[j+i//20*2, i%20].set_yticks([])\n",
    "    ax[1+i//20*2, 0].set_ylabel('Rec', fontsize=24)\n",
    "fig.tight_layout(pad=0)\n",
    "fig.savefig(PATH + 'CUBImgRecExtra.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7b6d3-1eac-4ba3-b4af-91ca83c1e5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents_h = []\n",
    "sents_emb = results['x_rec'][1].argmax(-1)\n",
    "for sent_emb in sents_emb:\n",
    "    sent_h = []\n",
    "    for j in sent_emb:\n",
    "        char = CUB_test.CUBtxt.i2w[str(int(j.item()))]\n",
    "        if char == '<eos>':\n",
    "            break\n",
    "        sent_h.append(char)\n",
    "    sents_h.append(' '.join(sent_h))\n",
    "\n",
    "sents = []\n",
    "for i, sent_h in enumerate(sents_h):\n",
    "    sent = []\n",
    "    for char in txts[i]['tok']:\n",
    "        if char == '<eos>':\n",
    "            break\n",
    "        sent.append(char)\n",
    "    sents.append(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b70cf-4dd7-4ef8-abc7-eff3c303e04b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent, sent_h in zip(sents, sents_h):\n",
    "    print('     ', sent)\n",
    "    print('Rec: ', sent_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2985b9-13c3-4ed7-82b2-602cc1d1d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH + 'CUBTxtRec.txt', 'w') as file:\n",
    "    for sent, sent_h in zip(sents, sents_h):\n",
    "        file.write('Ref: ' + sent + '\\n')\n",
    "        file.write('Rec: ' + sent_h + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cfe7e-5d06-4134-bdfb-0076ac3039f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
